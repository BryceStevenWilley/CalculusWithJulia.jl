{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applications with scalar functions"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia\nusing CalculusWithJulia.WeaveSupport\nusing Plots\nnothing"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section presents different applications of scalar functions.\nWe begin by loading our package allowing access to several useful packages of `Julia`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia\nusing Plots"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tangent planes, linearization\n\n\nConsider the case $f:R^2 \\rightarrow R$. We visualize $z=f(x,y)$ through a surface. At a point $(a, b)$, this surface, if $f$ is sufficiently smooth, can be approximated by a flat area, or a plane. For example, the Northern hemisphere of the earth, might be modeled simplistically by $z = \\sqrt{R^2 - (x^2 + y^2)}$ for some $R$ and with the origin at the earth's core. The ancient view of a \"flat earth,\" can be more generously seen as identifying this tangent plane with the sphere. More apt for current times, is the use of GPS coordinates to describe location. The difference between any two coordinates is technically a distance on a curved, nearly spherical, surface. But if the two points are reasonably closes (miles, not tens of miles) and accuracy isn't of utmost importance (i.e., not used for self-driving cars), then the distance can be found from the Euclidean distance formula, $\\sqrt{(\\Delta\\text{latitude})^2 + \\Delta\\text{longitude})^2}$. That is, as if the points were on a plane, not a curved surface.\n\nFor the univariate case, the tangent line has many different uses. Here we see the tangent plane also does.\n\n\n### Equation of the tangent plane\n\nThe partial derivatives have the geometric view of being the derivative of the univariate functions $f(\\vec\\gamma_x(t))$ and $f(\\vec\\gamma_y(t))$, where $\\vec\\gamma_x$ moves just parallel to the $x$ axis (e.g. $\\langle t + a, b\\rangle$). and $\\vec\\gamma_y$ moves just parallel to the $y$ axis. The partial derivatives then are slopes of tangent lines to each curve. The tangent plane, should it exist, should match both slopes at a given point. With this observation, we can identify it.\n\nConsider $f(\\vec\\gamma_x)$ at a point $(a,b)$. The path has a tangent vector, which has \"slope\" $\\frac{\\partial f}{\\partial x}$. and in the direction of the $x$ axis, but not the $y$ axis, as does this vector:  $\\langle 1, 0, \\frac{\\partial f}{\\partial x} \\rangle$. Similarly, this vector $\\langle 0, 1, \\frac{\\partial f}{\\partial y} \\rangle$ describes the tangent line to $f(\\vec\\gamma_y)$ a the point.\n\n\nThese two vectors will lie in the plane. The normal vector is found by their cross product:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SymPy\n@vars f_x f_y\nn = [1, 0, f_x] × [0, 1, f_y]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\vec{x} = \\langle a, b, f(a,b)$. The tangent plane at $\\vec{x}$ then is described by all vectors $\\vec{v}$ with $\\vec{n}\\cdot(\\vec{v} - \\vec{x})  = 0$. Using $\\vec{v} = \\langle x,y,z\\rangle$, we have:\n\n$$~\n[-\\frac{\\partial f}{\\partial x}, -\\frac{\\partial f}{\\partial y}, 1] \\cdot [x-a, y-b, z - f(a,b)] = 0,\n~$$\n\nor,\n\n$$~\nz = f(a,b) + \\frac{\\partial f}{\\partial x} (x-a) + \\frac{\\partial f}{\\partial y} (y-b),\n~$$\n\nwhich is more compactly expressed as\n\n$$~\nz = f(a,b) + \\nabla(f) \\cdot \\langle x-a, y-b \\rangle.\n~$$\n\nThis form would then generalize to scalar functions from $R^n \\rightarrow R$. This is consistent with the definition of $f$ being differentiable, where $\\nabla{f}$ plays the role of the slope in the formulas.\n\n\nThe following figure illustrates the above for the function $f(x,y) = 6 - x^2 - y^2$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = 6 - x^2 -y^2\nf(x)= f(x...)\n\na,b = 1, -1/2\n\n\n# draw surface\nxr = 7/4\nxs = ys = range(-xr, xr, length=100)\nsurface(xs, ys, f, legend=false)\n\n# visualize tangent plane as 3d polygon\npt = [a,b]\ntplane(x) = f(pt) + gradient(f)(pt) ⋅ (x - [a,b])\n\npts = [[a-1,b-1], [a+1, b-1], [a+1, b+1], [a-1, b+1], [a-1, b-1]]\nplot!(unzip([[pt..., tplane(pt)] for pt in pts])...)\n\n# plot paths in x and y direction through (a,b)\nγ_x(t) = pt + t*[1,0]\nγ_y(t) = pt + t*[0,1]\n\nplot_parametric_curve!(t -> [γ_x(t)..., (f∘γ_x)(t)], -xr-a, xr-a, linewidth=3)\nplot_parametric_curve!(t -> [γ_y(t)..., (f∘γ_y)(t)], -xr-b, xr-b, linewidth=3)\n\n# draw directional derivatives in 3d and normal\npt = [a, b, f(a,b)]\nfx, fy = gradient(f)(a,b)\narrow!(pt, [1, 0, fx], linewidth=3)\narrow!(pt, [0, 1, fy], linewidth=3)\narrow!(pt, [-fx, -fy, 1], linewidth=3) # normal\n\n# draw point in base, x-y, plane\npt = [a, b, 0]\nscatter!(unzip([pt])...)\narrow!(pt, [1,0,0], linestyle=:dash)\narrow!(pt, [0,1,0], linestyle=:dash)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternate forms\n\nThe equation for the tangent plane is often expressed in a more explicit form. For $n=2$, if we set $dx = x-a$ and $dy=y-a$, then the equation for the plane becomes:\n\n$$~\nf(a,b) + \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy,\n~$$\n\nwhich is a common form for the equation, though possibly confusing, as $\\partial x$ and $dx$ need to be distinguished. For $n > 2$, additional terms follow this pattern. This explicit form is helpful when doing calculations by hand, but much less so when working on the computer, say with `Julia`, as the representations using vectors (or matrices) can be readily implemented and their representation much closer to the formulas. For example, consider these two possible functions to find the tangent plane (returned as a function) at a point in 2 dimensions"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function tangent_plane(f, pt)\n  fx, fy = ForwardDiff.gradient(f, pt)\n  x -> f(x...) + fx * (x[1]-pt[1]) + fy * (x[2]-pt[2])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It isn't so bad, but as written, we specialized to the number of dimensions, used indexing,  and with additional dimensions, it clearly would get tedious to generalize. Using vectors, we might have:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function tangent_plane(f, pt)\n  ∇f = ForwardDiff.gradient(f, pt) # using a variable ∇f\n  x -> f(pt) + ∇f ⋅ (x - pt)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is much more like the compact formula and able to handle higher dimensions without rewriting.\n\n\n### Tangent plane for level curves\n\nConsider the surface described by $f(x,y,z) = c$, a constant. This is more general than surfaces described by $z = f(x,y)$. The concept of a tangent plane should still be applicable though. Suppose, $\\vec{\\gamma}(t)$ is a curve in the $x-y-z$ plane, then we have $(f\\circ\\vec\\gamma)(t)$ is a curve on the surface and its derivative is given by the chain rule through: $\\nabla{f}(\\vec\\gamma(t))\\cdot \\vec\\gamma'(t)$. But this composition is constantly the same value, so the derivative is $0$. This says that $\\nabla{f}(\\vec\\gamma(t))$ is *orthogonal* to $\\vec\\gamma'(t)$ for any curve. As these tangential vectors to $\\vec\\gamma$ lie in the tangent plane, the tangent plane can be characterized by having $\\nabla{f}$ as the normal.\n\nThis computation was previously done in two dimensions, and showed the gradient is orthogonal to the contour lines (and points in the direction of greatest ascent). It can be generalized to higher dimensions.\n\nThe surface $F(x,y,z) = z - f(x,y) = 0$ has gradient given by $\\langle\n-\\partial{f}/\\partial{x}, -\\partial{f}/\\partial{y}, 1\\rangle$, and as seen\nabove, this vector is normal to the tangent plane, so this\ngeneralization agrees on the easier case.\n\n\nFor clarity:\n\n* The scalar function $z = f(x,y)$ describes a surface, $(x,y,f(x,y)$; the gradient, $\\nabla{f}$ is $2$ dimensional and points in the direction of greatest ascent for the surface.\n* The scalar function $f(x,y,z)$ *also* describes a surface, through level curves $f(x,y,z) = c$, for some *constant* $c$. The gradient $\\nabla{f}$ is $3$ dimensional and *orthogonal* to the surface.\n\n\n##### Example\n\nLet $z = f(x,y) = \\sin(x)\\cos(x-y)$. Find an equation for the tangent plane at $(\\pi/4, \\pi/3)$.\n\nWe have many possible forms to express this in, but we will use the functional description:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x y\nvars = [x, y]\nf(x,y) = sin(x) * cos(x-y)\nf(x) = f(x...)\n\ngradf = diff.(f(x,y), vars)  # or use gradient(f, vars) or ∇((f,vars))\n\npt = [PI/4, PI/3]\ngradfa = subs.(gradf, x.=>pt[1], y.=>pt[2])\n\nf(pt) + gradfa ⋅ (vars - pt)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example\n\nA cylinder $f(x,y,z) = (x-a)^2 + y^2 = (2a)^2$ is intersected with a sphere $g(x,y,z) = x^2 + y^2 + z^2 = a^2$. Let $V$ be the line of intersection. (Viviani's curve). Let $P$ be a point on the curve. Describe the tangent to the curve.\n\nWe have the line of intersection will have tangent line  lying in the tangent plane to both surfaces. These two surfaces have normal vectors given by the gradient, or $\\vec{n}_1 = \\langle 2(x-a), 2y, 0 \\rangle$ and $\\vec{n}_2 = \\langle 2x, 2y, 2z \\rangle$. The cross product of these two vectors will lie in both tangent planes, so we have:\n\n$$~\nP + t (\\vec{n}_1 \\times \\vec{n}_2),\n~$$\n\nwill describe the tangent.\n\nThe curve may be described parametrically by $\\vec\\gamma(t) = a \\langle 1 + \\cos(t), \\sin(t), 2\\sin(t/2) \\rangle$. Let's see that the above is correct by verifying that the cross product of the tangent vector computed two ways is $0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "a = 1\ngamma(t) = a * [1 + cos(t), sin(t), 2sin(t/2) ]\nP = gamma(1/2)\nn1(x,y,z)= [2*(x-a), 2y, 0]\nn2(x,y,z) = [2x,2y,2z]\nn1(x) = n1(x...)\nn2(x) = n2(x...)\n\nt = 1/2\n(n1(gamma(t)) × n2(gamma(t))) × gamma'(t)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting level curves of $F(x,y,z) = c$\n\nThe `wireframe` plot can be used to a surface of the type `z=f(x,y)`, as previously illustrated. However we have no way of plotting $3$-dimensional implicit surfaces (of the type $F(x,y,z)=c$) as we do for $2$-dimensional implicit surfaces. (However, within the `Makie` plotting framework one is provided in `CalculusWithJulia`.) The following function, which is **not** part of `CalculusWithJulia` can be used for this task to make a surface akin to the wireframe plot. The basic idea is to slice an axis, by default the $z$ axis up and for each level plot the contours of $(x,y) \\rightarrow f(x,y,z)-c$, which becomes a $2$-dimensional problem. The function allows any of 3 different axes to be chosen to slice over, the default being just the $z$ axis."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import Contour # installed with the Plots package, so should be available\n               # import -- not using -- to avoid name collision\nfunction plot_implicit_surface(F, c=0;\n                       xlim=(-5,5), ylim=xlim, zlim=xlim,\n                       nlevels=25,         # number of levels in a direction\n                       slices=Dict(:z => :blue), # Dict(:x => :color, :y=>:color, :z=>:color)\n                       kwargs...          # passed to initial `plot` call\n                       )\n\n    _linspace(rng, n=150) = range(rng[1], stop=rng[2], length=n)\n\n    X1, Y1, Z1 = _linspace(xlim), _linspace(ylim), _linspace(zlim)\n\n    p = Plots.plot(;legend=false,kwargs...)\n\n    if :x ∈ keys(slices)\n        for x in _linspace(xlim, nlevels)\n            local X1 = [F(x,y,z) for y in Y1, z in Z1]\n            cnt = Contour.contours(Y1,Z1,X1, [c])\n            for line in Contour.lines(Contour.levels(cnt)[1])\n                ys, zs = Contour.coordinates(line) # coordinates of this line segment\n                plot!(p, x .+ 0 * ys, ys, zs, color=slices[:x])\n          end\n        end\n    end\n\n    if :y ∈ keys(slices)\n        for y in _linspace(ylim, nlevels)\n            local Y1 = [F(x,y,z) for x in X1, z in Z1]\n            cnt = Contour.contours(Z1,X1,Y1, [c])\n            for line in Contour.lines(Contour.levels(cnt)[1])\n                xs, zs = Contour.coordinates(line) # coordinates of this line segment\n                plot!(p, xs, y .+ 0 * xs, zs, color=slices[:y])\n            end\n        end\n    end\n\n    if :z ∈ keys(slices)\n        for z in _linspace(zlim, nlevels)\n            local Z1 = [F(x, y, z) for x in X1, y in Y1]\n            cnt = Contour.contours(X1, Y1, Z1, [c])\n            for line in Contour.lines(Contour.levels(cnt)[1])\n                xs, ys = Contour.coordinates(line) # coordinates of this line segment\n                plot!(p, xs, ys, z .+ 0 * xs, color=slices[:z])\n            end\n        end\n    end\n\n\n    p\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We demonstrate with an example from a February 14, 2019 article in the [New York Times](https://www.nytimes.com/2019/02/14/science/math-algorithm-valentine.html). It shows an equation for a \"heart,\" as the graphic will illustrate:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "a,b = 1,3\nf(x,y,z) = (x^2+((1+b)*y)^2+z^2-1)^3-x^2*z^3-a*y^2*z^3\n\nplot_implicit_surface(f, xlim=(-2,2), ylim=(-1,1), zlim=(-1,2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linearization\n\nThe tangent plane is the best \"linear approximation\" to a function at a point. \"Linear\" refers to mathematical properties of the tangent plane, but at a practical level it means easy to compute, as it will involve only multiplication and addition. \"Approximation\" is useful in that if a bit of error is an acceptable tradeoff for computational ease, the tangent plane may be used in place of the function. In the univariate case, this is known as linearization, and the tradeoff is widely used in the derivation of theoretical relationships, as well as in practice to get reasonable numeric values.\n\nFormally, this is saying:\n\n$$~\nf(\\vec{x}) \\approx f(\\vec{a}) + ∇f(\\vec{a}) ⋅ (\\vec{x} - \\vec{a}).\n~$$\n\nThe explicit meaning of $\\approx$ will be made clear when the generalization of Taylor's theorem is to be stated.\n\n\n##### Example: Linear approximation\n\nThe volume of a cylinder is $V=\\pi r^2 h$. It is thought a cylinder has $r=1$ and $h=2$. If instead, the amounts are $r=1.01, h=2.01$, what is the difference in volume?\n\nThat is, if $V(r,h) = \\pi r^2 h$, what is $V(1.01, 2.01) - V(1,2)$?\n\nWe can use linear approximation to see that this difference is *approximately* $\\nabla{V} \\cdot \\langle 0.01, 0.01 \\rangle$. This is:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "V(r, h) = pi * r^2 * h\nV(v) = V(v...)\na = [1,2]\ndx = [0.01, 0.01]\nForwardDiff.gradient(V, a) ⋅ dx   # or use ∇(V)(a)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exact difference can be computed:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "V(a + dx) - V(a)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example\n\nLet $f(x,y) = \\sin(\\pi x y^2)$. Estimate $f(1.1, 0.9)$.\n\nUsing linear approximation with $dx=0.1$ and $dy=-0.1$, this is\n\n$$~\nf(1,1) + \\nabla{f}(1,1) \\cdot \\langle 0.1, -0.1\\rangle,\n~$$\n\nwhere $f(1,1) = sin(\\pi) = 0$ and $\\nabla{f} = \\langle y^2\\cos(\\pi x y^2), \\cos(\\pi x y^2) 2y\\rangle = \\cos(\\pi x y^2)\\langle x,2y\\rangle$. So, the answer is:\n\n$$~\n0 + \\cos(\\pi) \\langle 1,2\\rangle\\cdot \\langle 0.1, -0.1 \\rangle =\n(-1)(0.1 - 2(0.1)) = 0.1.\n~$$\n\n##### Example\n\nA [piriform](http://www.math.harvard.edu/~knill/teaching/summer2011/handouts/32-linearization.pdf) is described by the quartic surface $f(x,y,z) = x^4 -x^3 + y^2+z^2 = 0$. Find the tangent line at the point $\\langle 2,2,2 \\rangle$.\n\nHere, $\\nabla{f}$ describes a *normal* to the tangent plane. The description of a plane may be  described by $\\hat{N}\\cdot(\\vec{x} - \\vec{x}_0) = 0$, where $\\vec{x}_0$ is identified with a point on the plane (the point $(2,2,2)$ here). With this, we have $\\hat{N}\\cdot\\vec{x} = ax + by + cz = \\hat{N}\\cdot\\langle 2,2,2\\rangle = 2(a+b+c)$. For ths problem, $\\nabla{f}(2,2,2) = \\langle a, b, c\\rangle$ is given by:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y,z) = x^4 -x^3 + y^2 + z^2\na, b,c = ∇(f)(2,2,2)\n\"$a x + $b y  + $c z = $([a,b,c] ⋅ [2,2,2])\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Newton's method to solve $f(x,y) = 0$ and $g(x,y)=0$.\n\n\nThe level curve $f(x,y)=0$ and the level curve $g(x,y)=0$ may intersect. Solving algebraically for the intersection may be difficult in most cases, though the linear case is not. (The linear case being the intersection of two lines).\n\nTo elaborate, consider two linear equations written in a general form:\n\n$$~\n\\begin{align}\nax + by &= u\\\\\ncx + dy &= v\n\\end{align}\n~$$\n\nA method to solve this by hand would be to solve for $y$ from one equation, replace this expression into the second equation and then solve for $x$. From there, $y$ can be found. A more advanced method expresses the problem in a matrix formulation of the form $Mx=b$ and solves that equation. This form of solving is implemented in `Julia`, through the \"backslash\" operator. Here is the general solution:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars a b c d u v\nM = [a b; c d]\nB = [u, v]\nM \\ B .|> simplify"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term $\\det(M) = ad-bc$ term is important, as evidenced by its appearance in the denominator of each term. When this is zero there is not a unique solution, as in the typical case.\n\n\n\nUsing Newton's method to solve for intersection points, uses\nlinearization of the surfaces to replace the problem to the\nintersection of level curves for tangent planes. This is the linear\ncase that can be readily solved. As with Newton's method for the\nunivariate case, the new answer is generally a better *approximation*\nto the answer, and the process is iterated to get a *good enough*\napproximation, as defined through some tolerance.\n\nConsider the functions $f(x,y) =2 - x^2 - y^2$ and\n$g(x,y) = 3 - 2x^2 - (1/3)y^2$. These graphs show their surfaces with the level sets for $c=0$ drawn and just the levels sets, showing they intersect in 4 places."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import Contour: contours, levels, level, lines, coordinates\n\n\nf(x,y) = 2 - x^2 - y^2\ng(x,y) = 3 - 2x^2 - (1/3)y^2\nxs = ys = range(-3, stop=3, length=100)\nzfs = [f(x,y) for x in xs, y in ys]\nzgs = [g(x,y) for x in xs, y in ys]\n\n\nps = Any[]\npf = surface(xs, ys, f, alpha=0.5, legend=false)\n\nfor cl in levels(contours(xs, ys, zfs, [0.0]))\n    for line in lines(cl)\n        _xs, _ys = coordinates(line)\n        plot!(pf, _xs, _ys, 0*_xs, linewidth=3, color=:blue)\n    end\nend\n\n\npg = surface(xs, ys, g, alpha=0.5, legend=false)\nfor cl in levels(contours(xs, ys, zgs, [0.0]))\n    for line in lines(cl)\n        _xs, _ys = coordinates(line)\n        plot!(pg, _xs, _ys, 0*_xs, linewidth=3, color=:red)\n    end\nend\n\npcnt = plot(legend=false)\nfor cl in levels(contours(xs, ys, zfs, [0.0]))\n    for line in lines(cl)\n        _xs, _ys = coordinates(line)\n        plot!(pcnt, _xs, _ys, linewidth=3, color=:blue)\n    end\nend\n\nfor cl in levels(contours(xs, ys, zgs, [0.0]))\n    for line in lines(cl)\n        _xs, _ys = coordinates(line)\n        plot!(pcnt, _xs, _ys, linewidth=3, color=:red)\n    end\nend\n\nl = @layout([a b c])\nplot(pf, pg, pcnt, layout=l)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look to find the intersection point near $(1,1)$ using Newton's method\n\n\nWe have by linearization:\n\n$$~\n\\begin{align}\nf(x,y) &\\approx f(x_n, y_n)  + \\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y \\\\\ng(x,y) &\\approx g(x_n, y_n)  + \\frac{\\partial g}{\\partial x}\\Delta x + \\frac{\\partial g}{\\partial y}\\Delta y,\n\\end{align}\n~$$\nwhere $\\Delta x = x- x_n$ and $\\Delta y = y-y_n$. Setting $f(x,y)=0$ and $g(x,y)=0$, leaves these two linear equations in $\\Delta x$ and $\\Delta y$:\n\n$$~\n\\begin{align}\n\\frac{\\partial f}{\\partial x} \\Delta x + \\frac{\\partial f}{\\partial y} \\Delta y &= -f(x_n, y_n)\\\\\n\\frac{\\partial g}{\\partial x} \\Delta x + \\frac{\\partial g}{\\partial y} \\Delta y &= -g(x_n, y_n).\n\\end{align}\n~$$\n\n\nOne step of Newton's method defines $(x_{n+1}, y_{n+1})$ to be the values $(x,y)$ that make the linearized functions about $(x_n, y_n)$ both equal to $\\vec{0}$.\n\n\n\nAs just described, we can use `Julia`'s `\\` operation to solve the above system of equations, if we express them in matrix form. With this, one step of Newton's method can be coded as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function newton_step(f, g, xn)\n    M = [ForwardDiff.gradient(f, xn)'; ForwardDiff.gradient(g, xn)']\n    b = -[f(xn), g(xn)]\n    Delta = M \\ b\n    xn + Delta\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We investigate what happens starting at $(1,1)$ after one step:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = 2 - x^2 - y^2\ng(x,y) = 3 - 2x^2 - (1/3)y^2\nf(v) = f(v...); g(v) = g(v...)\nx0 = [1,1]\nx1 = newton_step(f, g, x0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new function values are"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x1), g(x1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get better approximations by iterating. Here we hard code 4 more steps:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x2 = newton_step(f, g, x1)\nx3 = newton_step(f, g, x2)\nx4 = newton_step(f, g, x3)\nx5 = newton_step(f, g, x4)\nx5, f(x5), g(x5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that at the new point, `x5`, both functions  are basically the same value, $0$, so we have approximated the intersection point.\n\nFor nearby initial guesses and reasonable functions, Newton's method is *quadratic*, so should take few steps for convergence, as above.\n\nHere is a simplistic method to iterate $n$ steps:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function nm(f, g, x, n=5)\n    for i in 1:n\n      x = newton_step(f, g, x)\n    end\n    x\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example\n\nConsider the [bicylinder](https://blogs.scientificamerican.com/roots-of-unity/a-few-of-my-favorite-spaces-the-bicylinder/) the intersection of two perpendicular cylinders of the same radius. If the radius is $1$, we might express these by the functions:\n\n$$~\nf(x,y) = \\sqrt{1 - y^2}, \\quad g(x,y) = \\sqrt{1 - x^2}.\n~$$\n\nWe see that $(1,1)$, $(-1,1)$, $(1,-1)$ and $(-1,-1)$ are solutions to $f(x,y)=0$, $g(x,y)=0$ *and*\n$(0,0)$ is a solution to $f(x,y)=1$ and $g(x,y)=1$. What about a level like $1/2$, say?\n\nRather than work with $f(x,y) = c$ we solve $f(x,y)^2 = c^2$, as that will be avoid issues with the square root not being defined. Here is one way to solve:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "c = 1/2\nf(x,y) = 1 - y^2 - c^2\ng(x,y) = (1 - x^2) - c^2\nf(v) = f(v...); g(v) = g(v...)\nnm(f, g, [1/2, 1/3])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That $x=y$ is not so surprising, and in fact, this problem can more easily be solved analytically through $x^2 = y^2 = 1 - c^2$.\n\n\n\n\n\n\n## Implicit differentiation\n\nImplicit differentiation of an equation of two variables (say $x$ and $y$) is performed by *assuming* $y$ is a function of $x$ and when differentiating an expression with $y$, use the chain rule. For example, the slope of the tangent line, $dy/dx$, for the general ellipse $x^2/a + y^2/b = 1$ can be found through this calculation:\n\n$$~\n\\frac{d}{dx}(\\frac{x^2}{a} + \\frac{y^2}{b}) =\n\\frac{d}{dx}(1),\n~$$\n\nor, using $d/dx(y^2) = 2y dy/dx$:\n\n$$~\n\\frac{2x}{a} + \\frac{2y \\frac{dy}{dx}}{b} = 0.\n~$$\n\nFrom this, solving for $dy/dx$ is routine,  as the equation is linear in that unknown: $dy/dx = -(b/a)(x/y)$\n\nWith more variables, the same technique may be used. Say we have variables $x$, $y$, and $z$ in a relation like $F(x,y,z) = 0$. If we assume $z=z(x,y)$ for some differentiable function (we mention later what conditions will ensure this assumption is valid for some open set), then we can proceed as before, using the chain rule as necessary.\n\n\nFor example, consider the ellipsoid: $x^2/a + y^2/b + z^2/c = 1$. What is $\\partial z/\\partial x$ and $\\partial{z}/\\partial{y}$, as needed to describe the tangent plane as above?\n\n\nTo find $\\partial/\\partial{x}$ we have:\n\n$$~\n\\frac{\\partial}{\\partial{x}}(x^2/a + y^2/b + z^2/c) =\n\\frac{\\partial}{\\partial{x}}1,\n~$$\n\nor\n\n$$~\n\\frac{2x}{a} + \\frac{0}{b} + \\frac{2z\\frac{\\partial{z}}{\\partial{x}}}{c} = 0.\n~$$\n\nAgain the desired unknown is within a linear equation so can readily be solved:\n\n$$~\n\\frac{\\partial{z}}{\\partial{x}} = -\\frac{c}{a} \\frac{x}{z}.\n~$$\n\nA similar approach can be used for $\\partial{z}/\\partial{y}$.\n\n##### Example\n\nLet $f(x,y,z) = x^4 -x^3 + y^2 + z^2 = 0$ be a surface with point $(2,2,2)$. Find $\\partial{z}/\\partial{x}$ and $\\partial{z}/\\partial{y}$.\n\n\nTo find $\\partial{z}/\\partial{x}$ we have:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Z = SymFunction(\"Z\")\n@vars x y\nsolve(diff(x^4 -x^3 + y^2 + Z(x,y)^2, x), diff(Z(x,y),x))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, for $\\partial{z}/\\partial{y}$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Z = SymFunction(\"Z\")\n@vars x y\nsolve(diff(x^4 -x^3 + y^2 + Z(x,y)^2, y), diff(Z(x,y),y))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n\n\nFor a continuous univariate  function $f:R \\rightarrow R$ over an interval $I$ the question of finding a maximum or minimum value is aided by two theorems:\n\n* The Extreme Value Theorem, which states that if $I$ is closed (e.g, $I=[a,b]$) then $f$ has a maximum (minimum) value $M$ and there is at least one value  $c$ with $a \\leq c \\leq b$ with $M = f(x)$.\n\n* [Fermat](https://tinyurl.com/nfgz8fz)'s theorem on critical points, which states that if $f:(a,b) \\rightarrow R$ and $x_0$ is such that $a < x_0 < b$ and $f(x_0)$ is a *local* extremum. If $f$ is differentiable at $x_0$, then $f'(x_0) = 0$. That is, local extrema of $f$ happen at points where the derivative does not exist or is $0$ (critical points).\n\nThese two theorems provide an algorithm to find the extreme values of a continuous function over a closed interval: find the critical points, check these and the end points for the maximum and minimum value.\n\nThese checks can be reduced by two theorems that can classify critical points as local extrema, the first and second derivative tests.\n\n\nThese theorems have generalizations to scalar functions, allowing a\nsimilar study of extrema.\n\nFirst, we define a *local* maximum for $f:R^n \\rightarrow R$ over a\nregion $U$: a point $\\vec{a}$ in $U$ is a *local* maximum if\n$f(\\vec{a}) \\geq f(\\vec{u})$ for all $u$ in some ball about\n$\\vec{a}$. A *local* minimum would have $\\leq$ instead.\n\nAn *absolute* maximum over $U$, should it exist, would be $f(\\vec{a})$\nif there exists a value $\\vec{a}$ in $U$ with the property $f(\\vec{a})\n\\geq f(\\vec{u})$ for all $\\vec{u}$ in $U$.\n\nThe difference is the same as the one-dimensional case: local is a\nstatement about nearby points only, absolute a statement about all the\npoints in the specified set.\n\n> The [Extreme Value Theorem](https://tinyurl.com/yyhgxu8y) Let $f:R^n \\rightarrow R$ be continuous and defined on *closed* set $V$. Then $f$ has a minimum value $m$ and maximum value $M$ over $V$ and there exists at least two points $\\vec{a}$ and $\\vec{b}$ with $m = f(\\vec{a})$ and $M = f(\\vec{b})$.\n\n\n> [Fermat](https://tinyurl.com/nfgz8fz)'s theorem on critical points. Let $f:R^n \\rightarrow R$ be a continuous function defined on an *open* set $U$. If $x \\in U$ is a point where $f$ has a local extrema *and* $f$ is differentiable, then the gradient of $f$ at $x$ is $\\vec{0}$.\n\n\nCall a point in the domain of $f$ where the function is differentiable and the  gradient is zero a *stationary point* and a point in the domain where the function is either not differentiable or is a stationary point a *critical point*. The local extrema can only happen at critical points by Fermat.\n\nConsider the function $f(x,y) = e^{-(x^2 + y^2)/5} \\cos(x^2 + y^2)$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plotly()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y)= exp(-(x^2 + y^2)/5) * cos(x^2 + y^2)\nxs = ys = range(-4, 4, length=100)\nsurface(xs, ys, f, legend=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is differentiable and the gradient is given by:\n\n$$~\n\\nabla{f} = -2/5e^{-(x^2 + y^2)/5} (5\\sin(x^2 + y^2) + \\cos(x^2 + y^2)) \\langle x, y \\rangle.\n~$$\n\nThis is zero at the origin, or when $ 5\\sin(x^2 + y^2) = -\\cos(x^2 + y^2)$. The latter is $0$ on circles of radius $r$ where $5\\sin(r) = \\cos(r)$ or $r = \\tan^{-1}(-1/5) + k\\pi$ for $k = 1, 2, \\dots$. This matches the graph, where the extrema are on circles by symmetry. Imagine now, picking a value where the function takes a maximum and adding the tangent plane. As the gradient is $\\vec{0}$, this will be flat. The point at the origin will have the surface fall off from the tangent plane in each direction, whereas the other points, will have a circle where the tangent plane rests on the surface, but otherwise will fall off from the tangent plane. Characterizing this \"falling off\" will help to identify local maxima that are distinct.\n\n----\n\nNow consider the differentiable function $f(x,y) = xy$, graphed below with the projections of the $x$ and $y$ axes:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x*y\nxs = ys = range(-3, 3, length=100)\nsurface(xs, ys, f, legend=false)\n\nplot_parametric_curve!(t -> [t, 0, f(t, 0)], -4, 4, linewidth=5)\nplot_parametric_curve!(t -> [0, t, f(0, t)], -4, 4, linewidth=5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extrema happen at the edges of the region. The gradient is $\\nabla{f} = \\langle y, x \\rangle$. This is $\\vec{0}$ only at the origin. At the origin, were we to imagine a tangent plane, the surface falls off in one direction but falls *above* in the other direction. Such a point is referred to as a *saddle point*. A saddle point for a continuous $f:R^n \\rightarrow R$ would be a critical point, $\\vec{a}$ where for any ball with non-zero radius about $\\vec{a}$, there are values where the function is greater than $f(\\vec{a})$ and values where the function is less.\n\nTo identify these through formulas, and not graphically, we could try and use the first derivative test along all paths through $\\vec{a}$, but this approach is better at showing something isn't the case, like two paths to show non-continuity.\n\nThe generalization of the *second* derivative test is more concrete though. Recall, the second derivative test is about the concavity of the function at the critical point. When the concavity can be determined as non-zero, the test is conclusive; when the concavity is zero, the test is not conclusive. Similarly here:\n\n> The [second](https://en.wikipedia.org/wiki/Second_partial_derivative_test) Partial Derivative Test for $f:R^2 \\rightarrow R$.\n>\n> Assume the first and second partial derivatives of $f$ are defined and continuous; $\\vec{a}$ be a critical point of $f$; $H$ is the hessian matrix, $[f_{xx}\\quad f_{xy};f_{xy}\\quad f_{yy}]$, and $d = \\det(H) = f_{xx} f_{yy} - f_{xy}^2$ is the determinant of the Hessian matrix. Then:\n>\n> * The function $f$ has a local minimum at $\\vec{a}$ if $f_{xx} > 0$ *and* $d>0$,\n>\n> * The function $f$ has a local maximum at $\\vec{a}$ if $f_{xx} < 0$ *and* $d>0$,\n>\n> * The function $f$ has a saddle point at $\\vec{a}$ if $d < 0$,\n>\n> * Nothing can be said if $d=0$.\n\n----\n\n\nThe intuition behind a  proof follows. The case when $f_{xx} > 0$ and $d > 0$ uses a consequence of these assumptions that for any non-zero vector $\\vec{x}$ it *must* be that $x\\cdot(Hx) > 0$ ([positive definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)) *and* the quadratic approximation $f(\\vec{a}+d\\vec{x}) \\approx f(\\vec{a}) + \\nabla{f}(\\vec{a}) \\cdot d\\vec{x} + d\\vec{x} \\cdot (Hd\\vec{x}) = f(\\vec{a}) + d\\vec{x} \\cdot (Hd\\vec{x})$, so for any $d\\vec{x}$ small enough, $f(\\vec{a}+d\\vec{x}) \\geq f(\\vec{a})$. That is $f(\\vec{a})$ is a local minimum. Similarly, a proof for the local maximum follows by considering $-f$. Finally, if $d < 0$, then there are vectors, $d\\vec{x}$, for which  $ d\\vec{x} \\cdot (Hd\\vec{x})$ will have different signs, and along these vectors the function will be concave up/concave down.\n\nApply this to $f(x,y) = xy$ at $\\vec{a} = \\vec{0}$ we have $f_{xx} = f_{yy} = 0$ and $f_{xy} = 1$, so the determinant of the Hessian is $-1$. By the second partial derivative test, this critical point is a saddle point, as seen from the previous graph.\n\n\nApplying this to $f(x,y) = e^{-(x^2 + y^2)/5} \\cos(x^2 + y^2)$, we will use `SymPy` to compute the derivatives, as they get a bit involved:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) =  exp(-(x^2 + y^2)/5) * cos(x^2 + y^2)\n@vars x y\nH = sympy.hessian(f(x,y), [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is messy, but we only consider it at critical points. The point $(0,0)$ is graphically a local maximum. We can see from the Hessian, that the second partial derivative test will give the same characterization:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "H_00 = subs.(H, x.=>0, y.=>0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which satisfies:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "H_00[1,1] < 0 && det(H_00) > 0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now consider $\\vec{a} = \\langle \\sqrt{2\\pi + \\tan^{-1}(-1/5)}, 0 \\rangle$, a point on the first visible ring on the graph. The gradient vanishes here:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gradf = gradient(f(x,y), [x,y])\na = [sqrt(2PI + atan(-Sym(1)//5)), 0]\nsubs.(gradf, x.=> a[1], y.=> a[2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the test is *inconclusive*, as the determinant of the Hessian is $0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "a = [sqrt(PI + atan(-Sym(1)//5)), 0]\nH_a = subs.(H, x.=> a[1], y.=> a[2])\ndet(H_a)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The test is inconclusive, as it needs the function to \"fall away\" from the tangent plane in all directions, in this case, along a circular curve, the function touches the tangent plane, so it doesn't fall away.)\n\n##### Example\n\nCharacterize the critical points of $f(x,y) = 4xy - x^4 - y^4$.\n\nThe critical points may be found by solving when the gradient is $\\vec{0}$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x y real=true\nf(x,y) = 4x*y - x^4 - y^4\ngradf = gradient(f(x,y), [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pts = solve(gradf, [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are $3$ critical points. To classify them we need the sign of $f_{xx}$ and the determinant of the Hessian. We make a simple function to compute these, then apply it to each point using a comprehension:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "H = sympy.hessian(f(x,y), [x,y])\nfunction classify(H, pt)\n  Ha = subs.(H, x .=> pt[1], y .=> pt[2])\n  (det=det(Ha), f_xx=Ha[1,1])\nend\n[classify(H, pt) for pt in pts]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the first and third points have positive determinant and negative $f_{xx}$, so are relative maxima, and the second point has negative derivative, so is a saddle point. We graphically confirm this:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xs = ys = range(-3/2, 3/2, length=100)\nsurface(xs, ys, f, legend=false)\nscatter!(unzip([N.([pt...,f(pt...)]) for pt in pts])...)  # add each pt on surface"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example\n\nConsider the function $f(x,y) = x^2 + 3y^2 -x$ over the region $x^2 + y^2 \\leq 1$. This is a continuous function over a closed set, so will have both an absolute maximum and minimum. Find these from an investigation of the critical points and the boundary points.\n\nThe gradient is easily found: $\\nabla{f} = \\langle 2x - 1, 6y \\rangle$, and is $\\vec{0}$ only at $\\vec{a} = \\langle 1/2, 0 \\rangle$. The Hessian is:\n\n$$~\nH = \\left[\n\\begin{array}{}\n2 & 0\\\\\n0 & 6\n\\end{array}\n\\right].\n~$$\n\nAt $\\vec{a}$ this has positive determinant and $f_{xx} > 0$, so $\\vec{a}$ corresponds to a *local* minimum with values $f(\\vec{a}) = (1/2)^2 + 3(0) - 1/2 = -1/4$. The absolute maximum and minimum may occur here (well, not the maximum) or on the boundary, so that must be considered. In this case we can easily parameterize the boundary and turn this into the univariate case:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x^2 + 2y^2 - x\nf(v) = f(v...)\ngamma(t) = [cos(t), sin(t)]  # traces out x^2 + y^2 = 1 over [0, 2pi]\ng(t) = f(gamma(t))\n\nusing Roots\ncps = fzeros(g', 0, 2pi) # critical points of g\nappend!(cps, [0, 2pi])\nunique!(cps)\ng.(cps)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that maximum value is `2.25` and that the interior point, $\\vec{a}$, will be where the minimum value occurs. To see exactly where the maximum occurs, we look at the values of gamma:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "inds = [2,4]\ncps = cps[inds]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are multiples of $\\pi$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cps/pi"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have the maximum occurs at the angles $2\\pi/3$ and $4\\pi/3$. Here we visualize, using a hacky trick of assigning `NaN` values to the function to avoid plotting outside the circle:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x^2 + 2y^2 - x\nh(x,y) = f(x,y) * (x^2 + y^2 <= 1 ? 1 : NaN)\nxs = ys = range(-1,1, length=100)\nsurface(xs, ys, h)\n\nts = cps  # 2pi/3 and 4pi/3 by above\nxs, ys = cos.(ts), sin.(ts)\nzs = f.(xs, ys)\nscatter!(xs, ys, zs)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A contour plot also shows that some - and only one - extrema happens on the interior:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "h(x,y) = f(x,y) * (x^2 + y^2 <= 1 ? 1 : NaN)\nxs = ys = range(-1,1, length=100)\ncontour(xs, ys, h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extrema are identified by the enclosing regions, in this case the one around the point $(1/2, 0)$.\n\n\n##### Example: Steiner's problem\n\nThis is from [Strang](https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/MITRES_18_001_strang_13.pdf) p 506.\n\nWe have three points in the plane, $(x_1, y_1)$, $(x_2, y_2)$, and $(x_3,y_3)$. A point $p=(p_x, p_y)$ will have $3$ distances $d_1$, $d_2$, and $d_3$. Broadly speaking we want to minimize to find the point $p$ \"nearest\" the three fixed points within the triangle. Locating a facility so that it can service 3 separate cities might be one application. The answer depends on the notion of what measure of distance to use.\n\nIf the measure is the Euclidean distance, then $d_i^2 = (p_x - x_i)^2 + (p_y - y_i)^2$. If we sought to minimize $d_1^2 + d_2^2 + d_3^2$, then we would proceed as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x y x1 y1 x2 y2 x3 y3\nd2(p,x) = (p[1] - x[1])^2 + (p[2]-x[2])^2\nd2_1, d2_2, d2_3 = d2((x,y), (x1, y1)), d2((x,y), (x2, y2)), d2((x,y), (x3, y3))\nex = d2_1 + d2_2 + d2_3"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then find the gradient, and solve for when it is $\\vec{0}$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gradf = diff.(ex, [x,y])\nxstar = solve(gradf, [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only one critical point, so must be a minimum.\n\nWe confirm this by looking at the Hessian and noting $H_{11} > 0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "H = subs.(hessian(ex, [x,y]), x.=>xstar[x], y.=>xstar[y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it occurs at $(\\bar{x}, \\bar{y})$ where $\\bar{x} = (x_1 + x_2 + x_3)/3$ and $\\bar{y} = (y_1+y_2+y_3)/3$ - the averages of the three values - the critical point is an interior point of the triangle.\n\n\nAs mentioned by Strang, the real problem is to minimize $d_1 + d_2 + d_3$. A direct approach with `SymPy` - just replacing `d2` above with the square root` fails. Consider instead the gradient of $d_1$, say. To avoid square roots, this is taken implicitly from $d_1^2$:\n\n$$~\n\\frac{\\partial}{\\partial{x}}(d_1^2) = 2 d_1 \\frac{\\partial{d_1}}{\\partial{x}}.\n~$$\n\nBut computing directly from the expression yields $2(x - x_1)$ Solving, yields:\n\n$$~\n\\frac{\\partial{d_1}}{\\partial{x}} = \\frac{(x-x_1)}{d_1}, \\quad\n\\frac{\\partial{d_1}}{\\partial{y}} = \\frac{(y-y_1)}{d_1}.\n~$$\n\nThe gradient is then $(\\vec{p} - \\vec{x}_1)/\\|\\vec{p} - \\vec{x}_1\\|$, a *unit* vector, call it $\\hat{u}_1$. Similarly for $\\hat{u}_2$ and $\\hat{u}_3$.\n\nLet $f = d_1 + d_2 + d_3$. Then $\\nabla{f} = \\hat{u}_1 + \\hat{u}_2 + \\hat{u}_3$. At the minimum, the gradient is $\\vec{0}$, so the three unit vectors must cancel. This can only happen if the three make a \"peace\" sign with angles $120^\\circ$ between them.\nTo find the minimum then within the triangle, this point and the boundary must be considered, when this point falls outside the triangle.\n\nHere is a triangle, where the minimum would be within the triangle:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gr()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "us = [[cos(t), sin(t)] for t in (0, 2pi/3, 4pi/3)]\npolygon(ps) = unzip(vcat(ps, ps[1:1])) # easier way to plot a polygon\n\np = scatter([0],[0], markersize=2, legend=false, aspect_ratio=:equal)\n\nas = (1,2,3)\nplot!(p, polygon([a*u for (a,u) in zip(as, us)])...)\n[arrow!([0,0], a*u, alpha=0.5) for (a,u) in zip(as, us)]\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this triangle we find the Steiner point outside of the triangle."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "as = (1, -1, 3)\np = scatter([0],[0], markersize=2, legend=false)\np1, p2, p3 = ps = [a*u for (a,u) in zip(as, us)]\nplot!(p, polygon(ps)...)\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see where the minimum distance point is by constructing a plot. The minimum must be on the boundary, as the only point where the gradient vanishes is the origin, not in the triangle. The plot of the triangle has a contour plot of the distance function, so we see clearly that the minimum happens at the point `[0.5, -0.866025]`. On this plot, we drew the gradient at some points along the boundary. The gradient points in the direction of greatest increase - away from the minimum. That the gradient vectors have a non-zero projection onto the edges of the triangle in a direction pointing away from the point indicates that the function `d` would increase if moved along the boundary in that direction, as indeed it does."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function polygon(ps)\n    unzip(vcat(ps, ps[1:1]))\nend\n\neuclid_dist(x) = sum(norm(x-p) for p in ps)\neuclid_dist(x,y) = euclid_dist([x,y])\nxs = range(-1.5, 1.5, length=100)\nys = range(-3, 1.0, length=100)\n\np = plot(polygon(ps)..., linewidth=3, legend=false)\nscatter!(p, unzip(ps)..., markersize=3)\ncontour!(p, xs, ys, euclid_dist)\n\n# add some gradients along boundary\nli(t, p1, p2) = p1 + t*(p2-p1)  # t in [0,1]\nfor t in range(1/100, 1/2, length=3)\n    pt = li(t, p2, p3)\n    arrow!(pt, ForwardDiff.gradient(euclid_dist, pt))\n    pt = li(t, p2, p1)\n    arrow!(pt, ForwardDiff.gradient(euclid_dist, pt))\nend\n\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following graph, shows distance along each edge:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = plot(legend=false)\nfor i in 1:2, j in (i+1):3\n  plot!(p, t -> euclid_dist(li(t, ps[i], ps[j])), 0, 1)\nend\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The smallest value is when $t=0$ or $t=1$, so at one of the points, as `li` is defined above.\n\n\n##### Example: least squares\n\nWe know that two points determine a line. What happens when there are more than two points? This is common in statistics where a bivariate data set (pairs of points $(x,y)$) are summarized through a linear model $\\mu_{y|x} = \\alpha + \\beta x$, That is the average value for $y$ given a particular $x$ value is given through the equation of a line. The data is used to identify what the slope and intercept are for this line. We consider a simple case - $3$ points. The case of $n \\geq 3$ being similar.\n\nWe have a line $l(x) = \\alpha + \\beta(x)$ and three points $(x_1, y_1)$, $(x_2, y_2)$, and $(x_3, y_3)$. Unless these three points *happen* to be collinear, they can't possibly all lie on the same line. So to *approximate* a relationship by a line requires some inexactness. One measure of inexactness is the *vertical* distance to the line:\n\n$$~\nd1(\\alpha, \\beta) = |y_1 - l(x_1)| + |y_2 - l(x_2)| + |y_3 - l(x_3)|.\n~$$\n\nAnother might be the vertical squared distance to the line:\n\n\n$$~\nd2(\\alpha, \\beta) = (y_1 - l(x_1))^2 + (y_2 - l(x_2))^2 + (y_3 - l(x_3))^2 =\n(y1 - (\\alpha + \\beta x_1))^2 + (y3 - (\\alpha + \\beta x_3))^2 + (y3 - (\\alpha + \\beta x_3))^2\n~$$\n\nAnother might be the *shortest* distance to the line:\n\n$$~\nd3(\\alpha, \\beta) = \\frac{\\beta x_1 - y_1 + \\alpha}{\\sqrt{1 + \\beta^2}} + \\frac{\\beta x_2 - y_2 + \\alpha}{\\sqrt{1 + \\beta^2}} + \\frac{\\beta x_3 - y_3 + \\alpha}{\\sqrt{1 + \\beta^2}}.\n~$$\n\nThe method of least squares minimizes the second one of these. That is, it chooses $\\alpha$ and $\\beta$ that make the expression a minimum."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x1, x2, x3 = xs = Sym[\"x$i\" for i in 1:3]\ny1, y2, y3 = ys = Sym[\"y$i\" for i in 1:3]\nli(x, alpha, beta) =  alpha + beta * x\nd2(alpha, beta) = sum((y - li(x, alpha, beta))^2 for (y,x) in zip(ys, xs))\n\n@vars α β\nd2(α, β)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify $\\alpha$ and $\\beta$ we find the gradient:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "grad_d2 = diff.(d2(α, β), [α, β])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "out = solve(grad_d2, [α, β])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As found, the formulas aren't pretty. If $x_1 + x_2 + x_3 = 0$ they simplify. For example:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "subs(out[β], x1 + x2 + x3 => 0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\vec{x} = \\langle x_1, x_2, x_3 \\rangle$ and $\\vec{y} = \\langle\ny_1, y_2, y_3 \\rangle$ this is simply $(\\vec{x} \\cdot\n\\vec{y})/(\\vec{x}\\cdot \\vec{x})$, a formula that will generalize to\n$n > 3$. The assumption is not a restriction - it comes about by subtracting the mean,\n$\\bar{x} = (x_1 + x_2 + x_3)/3$, from each $x$ term (and similarly\nsubtract $\\bar{y}$ from each $y$ term). A process called \"centering.\"\n\nWith this observation, the formulas can be re-expressed through:\n\n$$~\n\\beta = \\frac{\\sum{x_i - \\bar{x}}(y_i - \\bar{y})}{\\sum(x_i-\\bar{x})^2},\n\\quad\n\\alpha = \\bar{y} - \\beta \\bar{x}.\n~$$\n\n\nRelative to the centered values, this may be viewed as a line through\n$(\\bar{x}, \\bar{y})$ with slope given by\n$(\\vec{x}-\\bar{x})\\cdot(\\vec{y}-\\bar{y}) / \\|\\vec{x}-\\bar{x}\\|$.\n\n\nAs an example, if the point are $(1,1), (2,3), (5,8)$ we get:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "[k => subs(v, x1=>1, y1=>1, x2=>2, y2=>3, x3=>5, y3=>8) for (k,v) in out]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent\n\nAs seen in the examples above, extrema may be identified analytically by solving for when the gradient is $0$. Here we discuss some numeric algorithms for finding extrema.\n\n\nAn algorithm to identify where a surface is at its minimum is [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). The gradient points in the direction of the steepest ascent of the surface and the negative gradient the direction of the steepest descent. To move to a minimum then, it make intuitive sense to move in the direction of the negative gradient. How far? That is a different question and one with different answers. Let's formulate the movement first, then discuss how far.\n\nLet $\\vec{x}_0$, $\\vec{x}_1$, $\\dots$, $\\vec{x}_n$ be the position of the algorithm for $n$ steps starting from an initial point $\\vec{x}_0$. The difference between these points is given by:\n\n$$~\n\\vec{x}_{n+1} = \\vec{x}_n - \\gamma \\nabla{f}(\\vec{x}_n),\n~$$\n\nwhere $\\gamma$ is some scaling factor for the gradient. The above quantifies the idea: to go from $\\vec{x}_n$ to $\\vec{x}_{n+1}$, move along $-\\nabla{f}$ by a certain amount.\n\nLet $\\Delta_x =\\vec{x}_{n}- \\vec{x}_{n-1}$ and $\\Delta_y =  \\nabla{f}(\\vec{x}_{n}) -  \\nabla{f}(\\vec{x}_{n-1})$ A variant of the Barzilai-Borwein method is to take $\\gamma_n = | \\Delta_x \\cdot \\Delta_y / \\Delta_y \\cdot \\Delta_y |$.\n\nTo illustrate, take $f(x,y) = -(x^2 + y^2) \\cdot e^{-(2x^2 + y^2)}$ and a starting point $\\langle 1, 1 \\rangle$. We have, starting with $\\gamma_0 = 1$ $5$ steps taken as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = -exp(-((x-1)^2 + 2(y-1/2)^2))\nf(x) = f(x...)\n\nxs = [[0.0, 0.0]] # we store a vector\ngammas = [1.0]\n\nfor n in 1:5\n    xn = xs[end]\n    gamma = gammas[end]\n    xn1 = xn - gamma * gradient(f)(xn)\n    dx, dy = xn1 - xn, gradient(f)(xn1) - gradient(f)(xn)\n    gamman1 = abs( (dx ⋅ dy) / (dy ⋅ dy) )\n\n    push!(xs, xn1)\n    push!(gammas, gamman1)\nend\n\n[(x, f(x)) for x in xs]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now visualize, using the `Contour` package to draw the contour lines in the $x-y$ plane:\n\n```echo=false; results=\"hidden\"\nplotly()\n```"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import Contour: contours, levels, level, lines, coordinates\n\nfunction surface_contour(xs, ys, f; offset=0)\n  p = surface(xs, ys, f, legend=false, fillalpha=0.5)\n\n  ## we add to the graphic p, then plot\n  zs = [f(x,y) for x in xs, y in ys]  # reverse order for use with Contour package\n  for cl in levels(contours(xs, ys, zs))\n    lvl = level(cl) # the z-value of this contour level\n    for line in lines(cl)\n        _xs, _ys = coordinates(line) # coordinates of this line segment\n        _zs = offset * _xs\n        plot!(p, _xs, _ys, _zs, alpha=0.5)        # add curve on x-y plane\n    end\n  end\n  p\nend\n\n\noffset = 0\nus = vs = range(-1, 2, length=100)\nsurface_contour(vs, vs, f, offset=offset)\npts = [[pt..., offset] for pt in xs]\nscatter!(unzip(pts)...)\nplot!(unzip(pts)..., linewidth=3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Newton's method for minimization\n\n\nA variant of Newton's method can be used to minimize a function $f:R^2 \\rightarrow R$. We look for points where both partial derivatives of $f$ vanish. Let $g(x,y) = \\partial f/\\partial x(x,y)$ and $h(x,y) = \\partial f/\\partial y(x,y)$. Then applying Newton's method, as above to solve simultaneously for when $g=0$ and $h=0$, we considered this matrix:\n\n$$~\nM = [\\nabla{g}'; \\nabla{h}'],\n~$$\n\nand had a step expressible in terms of the inverse of $M$ as $M^{-1} [g; h]$. In terms of the function $f$, this step is $H^{-1}\\nabla{f}$, where $H$ is the Hessian matrix. [Newton](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#Higher_dimensions)'s method then becomes:\n\n$$~\n\\vec{x}_{n+1} = \\vec{x}_n - [H_f(\\vec{x}_n]^{-1} \\nabla(f)(\\vec{x}_n).\n~$$\n\nThe Wikipedia page states where applicable, Newton's method converges much faster towards a local maximum or minimum than gradient descent.\n\n\n\n\nWe apply it to the task of characterizing the following function, which has a few different peaks over the region $[-3,3] \\times [-2,2]$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function peaks(x, y)\n    z = 3 * (1 - x)^2 * exp(-x^2 - (y + 1)^2)\n    z += -10 * (x / 5 - x^3 - y^5) * exp(-x^2 - y^2)\n    z += -1/3 * exp(-(x+1)^2 - y^2)\n    return z\nend\n\nxs = range(-3, stop=3, length=100)\nys = range(-2, stop=2, length=100)\nsurface(xs, ys, peaks)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we will solve for the critical points numerically, so consider the contour plot as well, as it shows better where the critical points are:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "contour(xs, ys, peaks)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over this region we see clearly 5 peaks or valleys: near $(0, 1.5)$, near $(1.2, 0)$, near $(0.2, -1.8)$, near $(-0.5, -0.8)$, and near $(-1.2, 0.2)$. To classify the $5$ critical points we need to first identify them, then compute the Hessian, and then, possibly compute $f_xx$ at the point. Here we do so for one of them using a numeric approach.\n\nFor concreteness, consider the peak or valley near $(0,1.5)$. We use Newton's method to numerically compute the critical point. The Newton step, specialized here is:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function newton_step(f, x)\n  M = ForwardDiff.hessian(f, x)\n  b = ForwardDiff.gradient(f, x)\n  x - M \\ b\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform 3 steps of Newton's method, and see that it has found a critical point."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "peaks(v) = peaks(v...)\nx = [0, 1.5]\nx = newton_step(peaks, x)\nx = newton_step(peaks, x)\nx = newton_step(peaks, x)\nx, ForwardDiff.gradient(peaks, x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hessian at this point is given by:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "H = ForwardDiff.hessian(peaks, x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From which we see:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fxx = H[1,1]\nd = det(H)\nfxx, d"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consequently we have a local maximum at this critical point."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "note(\"\"\" The `Optim.jl` package provides efficient implementations of these two numeric methods, and others. \"\"\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constrained optimization, Lagrange multipliers\n\n\nWe considered the problem of maximizing a function over a closed region. This maximum is achieved at a critical point *or* a boundary point. Investigating the critical points isn't so difficult and the second partial derivative test can help characterize the points along the way, but characterizing the boundary points usually involves parameterizing the boundary, which is not always so easy. However, if we put this problem into a more general setting a different technique becomes available.\n\nThe different setting is: maximize $f(x,y)$ subject to the constraint $g(x,y) = k$. The constraint can be used to describe the boundary used previously.\n\nWhy does this help? The key is something we have seen prior: If $g$ is differentiable, and we take $\\nabla{g}$, then it will point at directions *orthogonal* to the level curve $g(x,y) = 0$. (Parameterize the curve, then $(g\\circ\\vec{r})(t) = 0$ and so the chain rule has $\\nabla{g}(\\vec{r}(t)) \\cdot \\vec{r}'(t) = 0$.) For example, consider the function $g(x,y) = x^2 +2y^2 - 1$. The level curve $g(x,y) = 0$ is an ellipse. Here we plot the level curve, along with a few gradient vectors at points satisfying $g(x,y) = 0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gr()\n\ng(x,y) = x^2 + 2y^2 -1\ng(v) = g(v...)\n\nxs = range(-3, 3, length=100)\nys = range(-1, 4, length=100)\n\np = plot(aspect_ratio=:equal, legend=false)\ncontour!(xs, ys, g, levels=[0])\n\ngi(x) = sqrt(1/2*(1-x^2)) # solve for y in terms of x\npts = [[x, gi(x)] for x in (-3/4, -1/4, 1/4, 3/4)]\n\nfor pt in pts\n  arrow!(pt, ForwardDiff.gradient(g, pt) )\nend\n\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot we see the key property that $g$ is orthogonal to the level curve.\n\nNow consider $f(x,y)$, a function we wish to maximize. The gradient points in the direction of *greatest* increase, provided $f$ is smooth. We are interested in the value of this gradient along the level curve of $g$. Consider this figure representing a portion of the level curve, it's tangent, normal, the gradient of $f$, and the contours of $f$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "r(t) = [cos(t), sin(t)/2]\nplot_parametric_curve(r, pi/12, pi/3, legend=false, aspect_ratio=true, linewidth=3)\nT(t) = -r'(t) / norm(r'(t))\nNo(t) = T'(t) / norm(T'(t))\nt = pi/4\nlambda=1/10\nscatter!(unzip([r(t)])...)\narrow!(r(t), T(t)*lambda)\narrow!(r(t), No(t)* lambda)\n\nf(x,y)= x^2 + y^2\nf(v) = f(v...)\narrow!(r(t), lambda*ForwardDiff.gradient(f, r(t)))\n\nxs = range(0.5,1, length=100)\nys = range(0.1, 0.5, length=100)\ncontour!(xs, ys, f)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can identify the tangent, the normal, and subsequently the gradient of $f$. Is the point drawn a maximum of $f$ subject to the constraint $g$?\n\nThe answer is no, but why? By adding the contours of $f$, we see that moving along the curve from this point will increase or decrease $f$, depending on which direction we move in. As the *gradient* is the direction of greatest increase, we can see that the *projection* of the gradient on the tangent will point in a direction of *increase*.\n\n\nIt isn't just because the point picked was chosen to make a pretty picture, and not be a maximum. Rather, the fact that $\\nabla{f}$ has a non-trivial projection onto the tangent vector. What does it say if we move the point in the direction of this projection?\n\nThe gradient points in the direction of greatest increase. If we first move in one component of the gradient we will increase, just not as fast. This is because the directional derivative in the direction of the tangent will be non-zero. In the picture, if we were to move the point to the right along the curve $f(x,y)$ will increase.\n\nNow consider this figure at a different point of the figure:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "r(t) = [cos(t), sin(t)/2]\nplot_parametric_curve(r, -pi/6, pi/6, legend=false, aspect_ratio=true, linewidth=3)\nT(t) = -r'(t) / norm(r'(t))\nNo(t) = T'(t) / norm(T'(t))\nt = 0\nlambda=1/10\nscatter!(unzip([r(t)])...)\narrow!(r(t), T(t)*lambda)\narrow!(r(t), No(t)* lambda)\n\nf(x,y)= x^2 + y^2\nf(v) = f(v...)\narrow!(r(t), lambda*ForwardDiff.gradient(f, r(t)))\n\nxs = range(0.5,1.5, length=100)\nys = range(-0.5, 0.5, length=100)\ncontour!(xs, ys, f,  levels = [.7, .85, 1, 1.15, 1.3])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can still identify the tangent and normal directions.\nWhat is different about this point is that local movement on the constraint curve is also local movement on the contour line of $f$, so $f$ doesn't increase or decrease here, as it would if this point were an extrema along the contraint. The key to seeing this is the contour lines of $f$ are *tangent* to the constraint. The respective gradients are *orthogonal* to their tangent lines, and in dimension $2$, this implies they are parallel to each other.\n\n\n> The method of Lagrange Multipliers\n>\n> To optimize $f(x,y)$ subject to a constraint $g(x,y) = k$ we solve for all *simultaneous* solutions to\n>\n> $$~\n> \\begin{align}\n> \\nabla{f}(x,y) &= \\lambda \\nabla{g}(x,y), \\text{and}\\\\\n> g(x,y) &= k.\n> \\end{align}\n>~$$\n>\n> Then these possible points are evaluated to see if they are maxima or minima.\n\nThe method will not work if $\\nabla{g} = \\vec{0}$ or if $f$ and $g$ are not differentiable.\n\n----\n\n\n##### Example\n\nWe consider [again](\"../derivatives/optimization.html\") the problem of maximizing all rectangles subject to the perimeter being $20$. We have seen this results in a square. This time we use the Lagrange multiplier technique. We have two equations:\n\n$$~\nA(x,y) = xy, \\quad P(x,y) = 2x + 2y = 25.\n~$$\n\nWe see $\\nabla{A} = \\lambda \\nabla{P}$, or $\\langle y, x \\rangle = \\lambda \\langle 2, 2\\rangle$. We see the solution has $x = y$ and from the constraint $x=y = 5$.\n\nThis is clearly the maximum for this problem, though the Lagrange technique does not imply that, it only identifies possible extrema.\n\n\n##### Example\n\n\nWe can reverse the question: what are the ranges for the perimeter when the area is a fixed value of $25$? We have:\n\n$$~\nP(x,y) = 2x + 2y, \\quad A(x,y) = xy = 25.\n~$$\n\nNow we look for $\\nabla{P} = \\lambda \\nabla{A}$ and will get,  as the last example, that $\\langle 2, 2 \\rangle = \\lambda \\langle y, x\\rangle$. So $x=y$ and from the constraint $x=y=5$.\n\nHowever this is *not* the maximum perimeter, but rather the minimal perimeter. The maximum is $\\infty$, which comes about in the limit by considering long skinny rectangles.\n\n\n##### Example: A rephrasing\n\nAn slightly different formulation of the Lagrange method is to combine the equation and the constraint into one equation:\n\n$$~\nL(x,y,\\lambda) = f(x,y) - \\lambda (g(x,y)  -  k).\n~$$\n\nThe we have\n\n$$~\n\\begin{align}\n\\frac{\\partial L}{\\partial{x}} &= \\frac{\\partial{f}}{\\partial{x}} - \\lambda \\frac{\\partial{g}}{\\partial{x}}\\\\\n\\frac{\\partial L}{\\partial{y}} &= \\frac{\\partial{f}}{\\partial{y}} - \\lambda \\frac{\\partial{g}}{\\partial{y}}\\\\\n\\frac{\\partial L}{\\partial{\\lambda}} &= 0 + (g(x,y)  -  k).\n\\end{align}\n~$$\n\nBut if the Lagrange condition holds, each term is $0$, so Lagrange's method can be seen as solving for point $\\nabla{L} = \\vec{0}$. The optimization problem in two variables with a constraint becomes a problem of finding and classifying  zeros of a function with *three* variables.\n\n\nApply this to the optimization problem:\n\nFind the extrema of $f(x,y) = x^2 - y^2$ subject to the constraint $g(x,y) = x^2 + y^2 = 1$.\n\nWe have:\n\n$$~\nL(x, y, \\lambda) = f(x,y) - \\lambda(g(x,y) - 1)\n~$$\n\nWe can solve for $\\nabla{L} = \\vec{0}$ by hand, but we do so symbolically:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x y lambda\nf(x, y) = x^2 - y^2\ng(x, y) = x^2 + y^2\nk = 1\nL(x,y,lambda) = f(x,y) - lambda*(g(x,y) - k)\nds = solve(diff.(L(x,y,lambda), [x, y, lambda]))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has $4$ easy solutions, here are the values at each point:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "[f(d[x], d[y]) for d in ds]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "So $1$ is a maximum value and $-1$ a minimum value.\n\n##### Example: Dido's problem\n\nConsider a slightly different problem:  What shape should a rope (curve) of fixed length make to *maximize* the area between the rope and $x$ axis?\n\n\nLet $L$ be the length of the rope and suppose $y(x)$ describes the curve. Then we wish to\n\n$$~\n\\text{Maximize } \\int y(x) dx, \\quad\\text{subject to }\n\\int \\sqrt{1 + y'(x)^2} dx = L.\n~$$\n\nThe latter being the formula for arc length. This is very much like a optimization problem that Lagrange's method could help solve, but with one big difference: the answer is *not* a point but a *function*.\n\nThis is a variant of  [Dido](http://www.ams.org/publications/journals/notices/201709/rnoti-p980.pdf)'s problem, described by Bandle as\n\n> Dido’s Problem The Roman poet Publius Vergilius Maro (70–19 B.C.)\n> tells in his epic Aeneid the story of queen Dido, the daughter of the\n> Phoenician king of the 9th century B.C. After the assassination of her\n> husband by her brother she fled to a haven near Tunis. There she asked\n> the local leader, Yarb, for as much land as could be enclosed by the\n> hide of a bull. Since the deal seemed very modest, he agreed. Dido cut\n> the hide into narrow strips, tied them together and encircled a large\n> tract of land which became the city of Carthage. Dido faced the\n> following mathematical problem, which is also known as the\n> isoperimetric problem: Find among all curves of given length the one\n> which encloses maximal area.  Dido found intuitively the right answer.\n\nThe problem as stated above and method of solution follows notes by [Wang](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.1522&rep=rep1&type=pdf) though Bandle attributes the ideas  back to a 19-year old Lagrange in a letter to Euler.\n\n\nThe method of solution will be to *assume* we have the function and then characterize this function in such a way that it can be identified.\n\n\nFollowing Lagrange, we generalize the problem to the following: maximize $\\int_{x_0}^{x_1} f(x, y(x), y'(x)) dx$ subject to a constraint $\\int_{x_0}^{x_1} g(x,y(x), y'(x)) dx = K$. Suppose $y(x)$ is a solution.\n\nThe starting point is a *perturbation*: $\\hat{y}(x) = y(x) + \\epsilon_1 \\eta_1(x) + \\epsilon_2 \\eta_2(x)$. There are two perturbation terms, were only one term added, then the perturbation may make $\\hat{y}$ not satisfy the constraint, the second term is used to ensure the constraint is not violated. If $\\hat{y}$ is to be a possible solution to our problem, we would want $\\hat{y}(x_0) = \\hat{y}(x_1) = 0$, as it does for $y(x)$, so we *assume*  $\\eta_1$ and $\\eta_2$ satisfy this boundary condition.\n\nWith this notation, and fixing $y$ we can re-express the equations in terms ot $\\epsilon_1$ and $\\epsilon_2$:\n\n$$~\n\\begin{align}\nF(\\epsilon_1, \\epsilon_2) &= \\int f(x, \\hat{y}, \\hat{y}') dx =\n\\int f(x, y + \\epsilon_1 \\eta_1 + \\epsilon_2 \\eta_2, y' + \\epsilon_1 \\eta_1' + \\epsilon_2 \\eta_2') dx,\\\\\nG(\\epsilon_1, \\epsilon_2) &= \\int g(x, \\hat{y}, \\hat{y}') dx =\n\\int g(x, y + \\epsilon_1 \\eta_1 + \\epsilon_2 \\eta_2, y' + \\epsilon_1 \\eta_1' + \\epsilon_2 \\eta_2') dx.\n\\end{align}\n~$$\n\nThen our problem is restated as:\n\n$$~\n\\text{Maximize } F(\\epsilon_1, \\epsilon_2) \\text{ subject to }\nG(\\epsilon_1, \\epsilon_2) = L.\n~$$\n\nNow, Lagrange's method can be employed. This will be fruitful - even though we know the answer - it being $\\epsilon_1 = \\epsilon_2 = 0$!\n\nForging ahead, we compute $\\nabla{F}$ and $\\lambda \\nabla{G}$ and set $\\epsilon_1 = \\epsilon_2 = 0$ where the two  are equal. This will lead to a description of $y$ in terms of $y'$.\n\nLagrange's method has:\n\n$$~\n\\frac{\\partial{F}}{\\partial{\\epsilon_1}}(0,0) - \\lambda \\frac{\\partial{G}}{\\partial{\\epsilon_1}}(0,0) = 0, \\text{ and }\n\\frac{\\partial{F}}{\\partial{\\epsilon_2}}(0,0) - \\lambda \\frac{\\partial{G}}{\\partial{\\epsilon_2}}(0,0) = 0.\n~$$\n\nComputing just the first one, we have using the chain rule and assuming interchanging the derivative and integral is possible:\n\n$$~\n\\begin{align}\n\\frac{\\partial{F}}{\\partial{\\epsilon_1}}\n&= \\int \\frac{\\partial}{\\partial{\\epsilon_1}}(\nf(x, y + \\epsilon_1 \\eta_1 + \\epsilon_2 \\eta2, y' + \\epsilon_1 \\eta_1' + \\epsilon_2 \\eta_2')) dx\\\\\n&= \\int \\left(\\frac{\\partial{f}}{\\partial{y}} \\eta_1 + \\frac{\\partial{f}}{\\partial{y'}} \\eta_1'\\right) dx\\quad\\quad(\\text{from }\\nabla{f} \\cdot \\langle 0, \\eta_1, \\eta_1'\\rangle)\\\\\n&=\\int \\eta_1 \\left(\\frac{\\partial{f}}{\\partial{y}} - \\frac{d}{dx}\\frac{\\partial{f}}{\\partial{y'}}\\right) dx.\n\\end{align}\n~$$\n\nThe last line by integration by parts: $\\int u'(x) v(x) dx = (u \\cdot v)(x)\\mid_{x_0}^{x_1} - \\int u(x) \\frac{d}{dx} v(x) dx = - \\int u(x) \\frac{d}{dx} v(x) dx $. The last lines, as $\\eta_1 = 0$ at $x_0$ and $x_1$ by assumption. We get:\n\n$$~\n0 = \\int \\eta_1\\left(\\frac{\\partial{f}}{\\partial{y}} - \\frac{d}{dx}\\frac{\\partial{f}}{\\partial{y'}}\\right).\n~$$\n\nSimilarly were $G$ considered, we would a similar statement would be found. Setting $L(x, y, y') = f(x, y, y') - \\lambda g(x, y, y')$, the combination of terms gives:\n\n$$~\n0 = \\int \\eta_1\\left(\\frac{\\partial{L}}{\\partial{y}} - \\frac{d}{dx}\\frac{\\partial{L}}{\\partial{y'}}\\right) dx.\n~$$\n\nSince $\\eta_1$ is arbitrary save for its boundary conditions, under smoothness conditions on $L$ this will imply the rest of the integrand *must* be $0$. That is,\n\n> Euler-Lagrange equation with a constraint\n\nIf $y(x)$ is a maximizer of $\\int_{x_0}^{x_1} f(x, y, y')dx$ and sufficiently smooth over $[x0, x1]$ and $y(x)$ satisfies the constraint $\\int_{x_0}^{x_1} g(x, y, y')dx = K$ then there exists a constant $\\lambda$ such that $L = f -\\lambda g$ will satisfy:\n\n$$~\n\\frac{d}{dx}\\frac{\\partial{L}}{\\partial{y'}} - \\frac{\\partial{L}}{\\partial{y}}  = 0.\n~$$\n\nIf $\\partial{L}/\\partial{x} = 0$, this simplifies through the [Beltrami](https://en.wikipedia.org/wiki/Beltrami_identity) identity to:\n\n$$~\nL - y' \\frac{\\partial{L}}{\\partial{y'}} = C.\\quad\\quad\\text{Beltrami identity}\n~$$\n\n\n\n----\n\nFor Dido's problem, $f(x,y,y') = y$ and $g(x, y, y') = \\sqrt{1 + y'^2}$, so $L = y - \\lambda\\sqrt{1 + y'^2}$ will have $0$ partial derivative with respect to $x$.  Using the Beltrami identify we have:\n\n$$~\n(y - \\lambda\\sqrt{1 + y'^2}) - \\lambda y' \\frac{2y'}{2\\sqrt{1 + y'^2}} = C.\n~$$\n\nby multiplying through by the denominator and squaring to remove the square root, a quadratic equation in $y'^2$ can be found. This can be solved to give:\n\n$$~\ny' = \\frac{dy}{dx} = \\sqrt{\\frac{\\lambda^2 -(y + C)^2}{(y+C)^2}}.\n~$$\n\nThis can be integrated using the substitution $y + C = \\lambda \\cos\\theta$ to give: $-\\lambda\\int\\cos\\theta d\\theta = x + D$, $D$ some constant. That is:\n\n$$~\n\\begin{align}\nx + D &=  - \\lambda \\sin\\theta\\\\\ny + C &= \\lambda\\cos\\theta.\n\\end{align}\n~$$\n\nSquaring gives the equation of a circle: $(x +D)^2 + (y+C)^2 = \\lambda^2$.\n\nWe center and *rescale* the problem so that $x_0 = -1, x_1 = 1$. Then $L > 2$ as otherwise the rope is too short. From here, we describe the radius and center of the circle.\n\nWe have $y=0$ at $x=1$ and $-1$ giving:\n\n$$~\n\\begin{align}\n(-1 + D)^2 + (0 + C)^2 &= \\lambda^2\\\\\n(+1 + D)^2 + (0 + C)^2 &= \\lambda^2.\n\\end{align}\n~$$\n\nSquaring out and solving gives $D=0$, $1 + C^2 = \\lambda^2$. That is, an arc of circle with radius $1+C^2$ and centered at $(0, -C)$.\n\n$$~\nx^2 + (y + C)^2 = 1 + C^2.\n~$$\n\nNow to identify $C$ in terms of $L$. $L$ is the length of arc of circle of radius $r =\\sqrt{1 + C^2}$ and angle $2\\theta$, so $L = 2r\\theta$ But using the boundary conditions in the equations for $x$ and $y$ gives $\\tan\\theta = 1/C$, so $L = 2\\sqrt{1 + C^2}\\tan^{-1}(1/C)$ which can be solved for $C$ provided $L \\geq 2$.\n\n\n\n\n\n##### Example: more constraints\n\nConsider now the case of maximizing $f(x,y,z)$ subject to $g(x,y,z)=c$ and $h(x,y,z) = d$. Can something similar be said to characterize potential values for this to occur? Trying to describe where $g(x,y,z) = c$ and $h(x,y,z)=d$ in general will prove difficult. The easy case would be it the two equations were linear, in which case they would describe planes. Two non-parallel planes would intersect in a line. If the general case, imagine the surfaces locally replaced by their tangent planes, then their intersection would be a line, and this line would point in along the curve given by the intersection of the surfaces formed by the contraints. This line is similar to the tangent line in the 2-variable case. Now if $\\nabla{f}$, which points in the direction of greatest increase of $f$, had a non-zero projection onto this line, then moving the point in that direction along the line would increase $f$ and still leave the point following the contraints. That is, if there is a non-zero directional derivative the point is not a maximum.\n\n\nThe tangent planes are *orthogonal* to the vectors $\\nabla{g}$ and $\\nabla{h}$, so in this case parallel to $\\nabla{g} \\times \\nabla{h}$. The condition that $\\nabla{f}$ be *orthogonal* to this vector, means that $\\nabla{f}$ *must* sit in the plane described by $\\nabla{g}$ and $\\nabla{h}$ - the plane of orthogonal vectors to $\\nabla{g} \\times \\nabla{h}$. That is, this condition is needed:\n\n$$~\n\\nabla{f}(x,y,z) = \\lambda_1 \\nabla{g}(x,y,z) + \\lambda_2 \\nabla{h}(x,y,z).\n~$$\n\nAt a point satisfying the above, we would have the tangent \"plane\" of $f$ is contained in the intersection of the tangent \"plane\"s to $g$ and $h$.\n\n\n\n\n----\n\nConsider a curve given through the intersection of two expression $g_1(x,y,z) = x^2 + y^2 - z^2 = 0$ and $g_2(x,y,z) = x - 2z = 3$. What is the minimum distance to the origin along this curve?\n\nWe have $f(x,y,z) = \\text{distance}(\\vec{x},\\vec{0}) = \\sqrt{x^2 + y^2 + z^2}$, subject to the two constraints. As the square root is increasing, we can actually just consider $f(x,y,z) = x^2 + y^2 + z^2$, ignoring the square root. The Lagrange multiplier technique instructs us to look for solutions to:\n\n$$~\n\\langle 2x, 2y ,2x \\rangle = \\lambda_1\\langle 2x, 2y, -2z\\rangle + \\lambda_2 \\langle 1, 0, -2 \\rangle.\n~$$\n\nHere we use `SymPy`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x y z lambda1 lambda2\ng1(x, y, z) = x^2 + y^2 - z^2\ng2(x, y, z) = x - 2z - 3\nf(x,y,z)= x^2 + y^2 + z^2\nL(x,y,z,lambda1, lambda2) = f(x,y,z) - lambda1*(g1(x,y,z) - 0) - lambda2*(g2(x,y,z) - 0)\n\n∇L = diff.(L(x,y,z,lambda1, lambda2), [x, y, z,lambda1, lambda2])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before trying to solve for $\\nabla{L} = \\vec{0}$ we see from the second equation that *either* $\\lambda_1 = 1$ or $y = 0$. First we solve with $\\lambda_1 = 1$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "solve(subs.(∇L, lambda1 .=> 1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no real solutions. Next when $y = 0$ we get:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "out = solve(subs.(∇L, y .=> 0))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two solutions have values  yielding the extrema:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "[f(d[x], 0, d[z]) for d in out]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taylor's theorem\n\nTaylor's theorem for a univariate function states that if $f$ has $k+1$ derivatives in an open interval around $a$, $f^{(k)}$ is continuous between the closed interval from $a$ to $x$ then:\n\n$$~\nf(x) = \\sum_{j=0}^k \\frac{f^{j}(a)}{j!} (x-a)^k + R_k(x),\n~$$\n\nwhere $R_k(x) = f^{k+1}(\\xi)/(k+1)!(x-a)^{k+1}$ for some $\\xi$ between $a$ and $x$.\n\nThis theorem can be generalized to scalar functions, but the notation can be cumbersome.\nFollowing [Folland](https://sites.math.washington.edu/~folland/Math425/taylor2.pdf) we use *multi-index* notation. Suppose $f:R^n \\rightarrow R$, and let $\\alpha=(\\alpha_1, \\alpha_2, \\dots, \\alpha_n)$. Then define the following notation:\n\n$$~\n|\\alpha| = \\alpha_1 + \\cdots + \\alpha_n, \\quad\n\\alpha! = \\alpha_1!\\alpha_2!\\cdot\\cdots\\cdot\\alpha_n!,\\quad\n\\vec{x}^\\alpha = x_1^{\\alpha_1}x_2^{\\alpha_2}\\cdots x_n^{\\alpha^n}, \\quad\n\\partial^\\alpha f = \\partial_1^{\\alpha_1}\\partial_2^{\\alpha_2}\\cdots \\partial_n^{\\alpha_n} f =\n\\frac{\\partial^{|\\alpha|}f}{\\partial x_1^{\\alpha_1} \\partial x_2^{\\alpha_2} \\cdots \\partial x_n^{\\alpha_n}}.\n~$$\n\nThis notation makes many formulas from one dimension carry over to higher dimensions. For example, the binomial theorem says:\n\n$$~\n(a+b)^n = \\sum_{k=0}^n \\frac{n!}{k!(n-k)!}a^kb^{n-k},\n~$$\n\nand this becomes:\n\n$$~\n(x_1 + x_2 + \\cdots + x_n)^n = \\sum_{|\\alpha|=k} \\frac{k!}{\\alpha!} \\vec{x}^\\alpha.\n~$$\n\nTaylor's theorem then becomes:\n\nIf $f: R^n \\rightarrow R$ is sufficiently smooth ($C^{k+1}$) on an open convex set $S$ about $\\vec{a}$ then if $\\vec{a}$ and $\\vec{a}+\\vec{h}$ are in $S$,\n$$~\nf(\\vec{a} + \\vec{h}) = \\sum_{|\\alpha| \\leq k}\\frac{\\partial^\\alpha f(\\vec{a})}{\\alpha!}\\vec{h}^\\alpha + R_{\\vec{a},k}(\\vec{h}),\n~$$\nwhere $R_{\\vec{a},k} = \\sum_{|\\alpha|=k+1}\\partial^\\alpha \\frac{f(\\vec{a} + c\\vec{h})}{\\alpha!} \\vec{h}^\\alpha$ for some $c$ in $(0,1)$.\n\n##### Example\n\nThe elegant notation masks what can be complicated expressions. Consider the simple case $f:R^2 \\rightarrow R$ and $k=2$. Then this says:\n\n$$~\nf(x + dx, y+dy) = f(x, y) + \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy\n+ \\frac{\\partial^2 f}{\\partial x^2} \\frac{dx^2}{2} +  2\\frac{\\partial^2 f}{\\partial x\\partial y} \\frac{dx dy}{2} +\n+ \\frac{\\partial^2 f}{\\partial y^2} \\frac{dy^2}{2} + R_{\\langle x, y \\rangle, k}(\\langle dx, dy \\rangle).\n~$$\n\nUsing $\\nabla$ and $H$ for the Hessian and $\\vec{x} = \\langle x, y \\rangle$ and $d\\vec{x} = \\langle dx, dy \\rangle$, this can be expressed as:\n\n$$~\nf(\\vec{x} + d\\vec{x}) = f(\\vec{x}) + \\nabla{f} \\cdot d\\vec{x} +  d\\vec{x} \\cdot (H d\\vec{x}) +R_{\\vec{x}, k}d\\vec{x}.\n~$$\n\nAs for $R$, the full term involves terms for $\\alpha = (3,0), (2,1), (1,2)$, and $(0,3)$. Using $\\vec{a} = \\langle x, y\\rangle$ and $\\vec{h}=\\langle dx, dy\\rangle$:\n\n$$~\n\\frac{\\partial^3 f(\\vec{a}+c\\vec{h})}{\\partial x^3} \\frac{dx^3}{3!}+\n\\frac{\\partial^3 f(\\vec{a}+c\\vec{h})}{\\partial x^2\\partial y} \\frac{dx^2 dy}{2!1!} +\n\\frac{\\partial^3 f(\\vec{a}+c\\vec{h})}{\\partial x\\partial y^2} \\frac{dxdy^2}{1!2!} +\n\\frac{\\partial^3 f(\\vec{a}+c\\vec{h})}{\\partial y^3} \\frac{dy^3}{3!}.\n~$$\n\nThe exact answer is usually not as useful as the bound: $|R| \\leq M/(k+1)! \\|\\vec{h}\\|^{k+1}$, for some finite constant $M$.\n\n\n##### Example\n\nWe can encode multiindices using `SymPy`. The basic definitions are fairly straightforward using `zip` to pair variables with components of $\\alpha$. We define a new type so that we can overload the familiar notation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct MultiIndex\n  alpha::Vector{Int}\n  end\nBase.show(io::IO, α::MultiIndex) = println(io, \"α = ($(join(α.alpha, \", \")))\")\n\n## |α| = α_1 + ... + α_m\nBase.length(α::MultiIndex) = sum(α.alpha)\n\n## factorial(α) computes α!\nBase.factorial(α::MultiIndex) = prod(factorial(Sym(a)) for a in α.alpha)\n\n## x^α = x_1^α_1 * x_2^α^2 * ... * x_n^α_n\nimport Base: ^\n^(x, α::MultiIndex) = prod(u^a for (u,a) in zip(x, α.alpha))\n\n## ∂^α(ex) = ∂_1^α_1 ∘ ∂_2^α_2 ∘ ... ∘ ∂_n^α_n (ex)\npartial(ex::SymPy.SymbolicObject, α::MultiIndex, vars=free_symbols(ex)) = diff(ex, zip(vars, α.alpha)...)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars w x y z\nalpha = MultiIndex([1,2,1,3])\nlength(alpha)  # 1 + 2 + 1 + 3=7\n[1,2,3,4]^alpha\nex = x^3 * cos(w*y*z)\npartial(ex, alpha, [w,x,y,z])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The remainder term needs to know information about sets like $|\\alpha| =k$. This is a combinatoric problem, even to identify the length. Here we define an iterator to iterate over all possible MultiIndexes. This is low level, and likely could be done in a much better style, so shouldn't be parsed unless there is curiosity. It manually chains together iterators."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct MultiIndices\n    n::Int\n    k::Int\nend\n\nfunction Base.length(as::MultiIndices)\n  n,k = as.n, as.k\n  n == 1 && return 1\n  sum(length(MultiIndices(n-1, j)) for j in 0:k)  # recursively identify length\nend\n\nfunction Base.iterate(alphas::MultiIndices)\n    k, n = alphas.k, alphas.n\n    n == 1 && return ([k],(0, MultiIndices(0,0), nothing))\n\n    m = zeros(Int, n)\n    m[1] = k\n    betas = MultiIndices(n-1, 0)\n    stb = iterate(betas)\n    st = (k, MultiIndices(n-1, 0), stb)\n    return (m, st)\nend\n\nfunction Base.iterate(alphas::MultiIndices, st)\n\n    st == nothing && return nothing\n    k,n = alphas.k, alphas.n\n    k == 0 && return nothing\n    n == 1 && return nothing\n\n    # can we iterate the next on\n    bk, bs, stb = st\n\n    if stb==nothing\n        bk = bk-1\n        bk < 0 && return nothing\n        bs = MultiIndices(bs.n, bs.k+1)\n        val, stb = iterate(bs)\n        return (vcat(bk,val), (bk, bs, stb))\n    end\n\n    resp = iterate(bs, stb)\n    if resp == nothing\n        bk = bk-1\n        bk < 0 && return nothing\n        bs = MultiIndices(bs.n, bs.k+1)\n        val, stb = iterate(bs)\n        return (vcat(bk, val), (bk, bs, stb))\n    end\n\n    val, stb = resp\n    return (vcat(bk, val), (bk, bs, stb))\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a vector, not a `MultiIndex`. Here we get all multiindices in two variables of size $3$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "collect(MultiIndices(2, 3))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get all of size $3$ or less, we could do something like this:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "union((collect(MultiIndices(2, i)) for i in 0:3)...)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the computational complexity. Suppose we had $3$ variables and were interested in the error for order $4$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "k = 4\nlength(MultiIndices(3, k+1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to see how compact the notation issue, suppose $f:R^3 \\rightarrow R$, we have the third-order Taylor series expands to 20 terms as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "F = SymFunction(\"F\")\na = Sym[\"a$i\" for i in 1:3]    # n=3\ndx = Sym[\"dx$i\" for i in 1:3]\n\nsum(partial(F(a...), α, a) / factorial(α) * dx^α for k in 0:3 for α in MultiIndex.(MultiIndices(3, k)))  # 3rd order"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n\n###### Question\n\nLet $f(x,y) = \\sqrt{x + y}$. Find the tangent plane approximation for $f(2.1, 2.2)$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = sqrt(x + y)\nf(v) = f(v...)\npt = [2,2]\ndxdy = [.1, .2]\nval = f(pt) + dot(ForwardDiff.gradient(f, pt), dxdy)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLet $f(x,y,z) = xy + yz + zx$. Using a *linear approximation* estimate $f(1.1, 1.0, 0.9)$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y,z) = x*y + y*z + z*x\nf(v) = f(v...)\npt = [1,1,1]\ndx = [0.1, 0.0, -0.1]\nval = f(pt) + ∇(f)(pt) ⋅ dx\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLet $f(x,y,z) = xy + yz + zx - 3$. What equation describes the tangent approximation at $(1,1,1)$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y,z) = x*y + y*z + z*x - 8\nf(v) = f(v...)\npt = [1,1,1]\nn = ∇(f)(pt)\nd = dot(n, pt)\nchoices = [L\"x + y + z = 3\",\nL\"2x + y - 2z = 1\",\nL\"x + 2y + 3z = 6\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n([Knill](http://www.math.harvard.edu/~knill/teaching/summer2018/handouts/week4.pdf)) Let $f(x,y) = xy + x^2y + xy^2$.\n\nFind the gradient of $f$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [L\"\\langle 2xy + y^2 + y, 2xy + x^2 + x\\rangle\",\nL\"y^2 + y, x^2 + x\",\nL\"\\langle 2y + y^2, 2x + x^2\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this the Hessian of $f$?\n\n$$~\n\\left[\\begin{matrix}2 y & 2 x + 2 y + 1\\\\2 x + 2 y + 1 & 2 x\\end{matrix}\\right]\n~$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point $(-1/3, -1/3)$ is a solution to the $\\nabla{f} = 0$. What is the *determinant*, $d$, of the Hessian at this point?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x*y + x*y^2 + x^2 * y\nf(v) = f(v...)\nval = det(ForwardDiff.hessian(f, [-1/3, -1/3]))\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is true of $f$ at $(-1/3, 1/3)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"The function $f$ has a local minimum, as $f_{xx} > 0$ and $d >0$\",\nL\"The function $f$ has a local maximum, as $f_{xx} < 0$ and $d >0$\",\nL\"The function $f$ has a saddle point, as $d  < 0$\",\nL\"Nothing can be said, as $d=0$\"\n]\nans = 2\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Question\n\n([Knill](http://www.math.harvard.edu/~knill/teaching/summer2018/handouts/week4.pdf)) Let the Tutte polynomial be $f(x,y) = x + 2x^2 + x^3 + y + 2xy + y^2$.\n\nDoes this accurately find the gradient of $f$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x + 2x^2 + x^3 + y + 2x*y + y^2\n@vars x y real=true\ngradf = gradient(f(x,y), [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many answers does this find to $\\nabla{f} = \\vec{0}$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "solve(gradf, [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "numericq(2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hessian is found by"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sympy.hessian(f(x,y), [x,y])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is true of $f$ at $(-1/3, 1/3)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"The function $f$ has a local minimum, as $f_{xx} > 0$ and $d >0$\",\nL\"The function $f$ has a local maximum, as $f_{xx} < 0$ and $d >0$\",\nL\"The function $f$ has a saddle point, as $d  < 0$\",\nL\"Nothing can be said, as $d=0$\",\nL\"The test does not apply, as $\\nabla{f}$ is not $0$ at this point.\"\n]\nans = 3\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is true of $f$ at $(0, -1/2)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"The function $f$ has a local minimum, as $f_{xx} > 0$ and $d >0$\",\nL\"The function $f$ has a local maximum, as $f_{xx} < 0$ and $d >0$\",\nL\"The function $f$ has a saddle point, as $d  < 0$\",\nL\"Nothing can be said, as $d=0$\",\nL\"The test does not apply, as $\\nabla{f}$ is not $0$ at this point.\"\n]\nans = 1\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is true of $f$ at $(1/2, 0)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"The function $f$ has a local minimum, as $f_{xx} > 0$ and $d >0$\",\nL\"The function $f$ has a local maximum, as $f_{xx} < 0$ and $d >0$\",\nL\"The function $f$ has a saddle point, as $d  < 0$\",\nL\"Nothing can be said, as $d=0$\",\nL\"The test does not apply, as $\\nabla{f}$ is not $0$ at this point.\"\n]\nans = 5\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n(Strang p509) Consider the quadratic function $f(x,y) = ax^2 + bxy +cy^2$. Since the second partial derivative test is essentially done by replacing the function at a critical point by a quadratic function, understanding this $f$ is of some interest.\n\nIs this the Hessian of $f$?\n\n$$~\n\\left[\n\\begin{array}{}\n2a & 2b\\\\\n2b  & 2c\n\\end{array}\n\\right]\n~$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or is this the Hessian of $f$?\n\n$$~\n\\left[\n\\begin{array}{}\n2ax & by\\\\\nbx  & 2cy\n\\end{array}\n\\right]\n~$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain why $ac - b^2$ is of any interest here:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices =[\n\"It is the determinant of the Hessian\",\nL\"It isn't, $b^2-4ac$ is from the quadratic formula\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which condition on $a$, $b$, and $c$ will ensure a *local maximum*:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"That $a>0$ and $ac-b^2 > 0$\",\nL\"That $a<0$ and $ac-b^2 > 0$\",\nL\"That $ac-b^2 < 0$\"\n]\nans = 2\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which condition on $a$, $b$, and $c$ will ensure a saddle point?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"That $a>0$ and $ac-b^2 > 0$\",\nL\"That $a<0$ and $ac-b^2 > 0$\",\nL\"That $ac-b^2 < 0$\"\n]\nans = 3\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLet $f(x,y) = e^{-x^2 - y^2} (2x^2 + y^2)$. Use Lagrange's method to find the absolute maximum and absolute minimum over $x^2 + y^2 = 3$.\n\n\nIs $\\nabla{f}$ given by the following?\n\n$$~\n\\nabla{f} =2 e^{-x^2 - y^2} \\langle x(2 - 2x^2 - y^2), y(1 - 2x^2 - y^2)\\rangle.\n~$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which vector is orthogonal to the contour line $x^2 + y^2 = 3$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"\\langle 2x, 2y\\rangle\",\nL\"\\langle 2x, y^2\\rangle\",\nL\"\\langle x^2, 2y \\rangle\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the form of the gradient of the constraint, finding when $\\nabla{f} = \\lambda \\nabla{g}$ is the same as identifying when this ratio $|f_x/f_y|$ is $1$. The following solves for this by checking each point on the constraint:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = exp(-x^2-y^2) * (2x^2 + y^2)\nf(v) = f(v...)\nr(t) = 3*[cos(t), sin(t)]\nrat(x) = abs(x[1]/x[2]) - 1\nfn = rat ∘ ∇(f) ∘ r\nts = fzeros(fn, 0, 2pi)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these points, what is the largest value on the boundary?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "val = maximum((f∘r).(ts))\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.0"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
