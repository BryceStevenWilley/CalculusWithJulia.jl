{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taylor Polynomials and other Approximating Polynomials"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia\nusing CalculusWithJulia.WeaveSupport\nusing Plots\nnothing"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tangent line was seen to be the \"best\" linear approximation to a\nfunction at a point $c$. Approximating a function by a linear function\ngives an easier to use approximation at the expense of accuracy. It\nsuggests a tradeoff between ease and accuracy. Is there a way to gain more accuracy at the expense of ease?\n\nQuadratic functions are still fairly easy to work with. Is it possible to find the best \"quadratic\"\napproximation to a function at a point $c$.\n\nMore generally, for a given $n$, what would be the best polynomial of\ndegree $n$ to approximate $f(x)$ at $c$?\n\nWe will see in this section how the Taylor polynomial answers these\nquestions, and is the appropriate generalization of the tangent\nline approximation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "###{{{taylor_animation}}}\npyplot()\nfig_size = (600, 400)\n\ntaylor(f, x, c, n) = series(f, x, c, n+1).removeO()\nfunction make_taylor_plot(u, a, b, k)\n    k = 2k\n    plot(u, a, b, title=\"plot of T_$k\", linewidth=5, legend=false, size=fig_size, ylim=(-2,2.5))\n    if k == 1\n        plot!(zero, range(a, stop=b, length=100))\n    else\n        plot!(taylor(u, x, 0, k), range(a, stop=b, length=100))\n    end\nend\n\n\n\n@vars x\nu = 1 - cos(x)\na, b = -2pi, 2pi\nn = 8\nanim = @animate for i=1:n\n    make_taylor_plot(u, a, b, i)\nend\n\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 1)\n\n\ncaption = L\"\"\"\n\nIllustration of the Taylor polynomial of degree $k$, $T_k(x)$, at $c=0$ and its graph overlayed on that of the function $1 - \\cos(x)$.\n\n\"\"\"\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The secant line and the tangent line\n\nTo motivate, we have two related formulas. Suppose we have a function\n$f(x)$ which is defined in a neighborhood of $c$ and has as many\nderivatives as we care to take at $c$.\n\n\nThe secant line connecting $(c, f(c))$ and $(c+h, f(c+h))$ for a value of $h>0$ is given in point-slope form by\n\n$$~\nsl(x) = f(c) + \\frac{(f(c+h) - f(c))}{h} \\cdot (x-c).\n~$$\n\nThe slope is the familiar approximation to the derivative: $(f(c+h)-f(c))/h$.\n\nThe *tangent line* to the graph of $f(x)$ at $x=c$ is described by the function\n\n$$~\ntl(x) = f(c) + f'(c) \\cdot(x - c).\n~$$\n\nThe secant line is important here, as it approximates the tangent\nline, which in turn is important, as it is the linear function that\nbest approximates the function at the point $(c, f(c))$. This is\nquantified by the *Mean Value Theorem* which states that there exists\nsome $\\xi$ between $x$ and $c$ for which:\n\n$$~\nf(x) - tl(x) = \\frac{f''(\\xi)}{2} \\cdot (x-c)^2.\n~$$\n\n\n(The term \"best\" is deserved, as any other straight line will differ\nat least in an $(x-c)$ term, which in general is larger than an\n$(x-c)^2$ term for $x$ \"near\" $c$.)\n\n\nThe secant line also has an interpretation that will generalize - it is the smallest order polynomial that goes through the points $(c,f(c))$ and $(c+h, f(c+h))$. This is obvious from the construction - as this is how the slope is derived - but from the formula itself requires showing $tl(c) = f(c)$ and $tl(c+h) = f(c+h)$. The former is straightforward, as $(c-c) = 0$, so clearly $tl(c) = f(c)$. The latter requires a bit of algebra.\n\n\nNow, we take a small detour to define some notation. Instead of\nwriting our two points as $c$ and $c+h$, let's use $x_0$ and\n$x_1$. For any set of points $x_0, x_1, \\dots, x_n$,\ndefine the **divided differences** of $f$ inductively, as follows:\n\n$$~\n\\begin{align}\nf[x_0] &= f(x_0) \\\\\nf[x_0, x_1] &= \\frac{f[x_1] - f[x_0]}{x_1 - x_0}\\\\\n\\cdots &\\\\\nf[x_0, x_1, x_2, \\dots, x_n] &= \\frac{f[x_1, \\dots, x_n] - f[x_0, x_1, x_2, \\dots, x_{n-1}]}{x_n - x_0}.\n\\end{align}\n~$$\n\n\nWe see the first two values look familiar, and to generate more we just take certain ratios akin to those formed when finding a secant line.\n\n\nWith this notation the secant line can be re-expressed as:\n\n$$~\nsl(x) = f[c] + f[c, c+h] \\cdot (x-c)\n~$$\n\nIf we think of $f[c, c+h]$ as an approximate *first* derivative, we\nhave an even stronger parallel between a secant line $x=c$ and the\ntangent line at $x=c$.\n\nTo see that this isn't far-fetched, we investigate with `SymPy`. First we create a recursive function to compute the divided differences:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "divided_differences(f, x) = f(x)\nfunction divided_differences(f, x, xs...)\n    xs = sort(vcat(x, xs...))\n    (divided_differences(f, xs[2:end]...) - divided_differences(f, xs[1:end-1]...)) / (xs[end] - xs[1])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `SymPy` we have, using $u$ in place of $f$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia   # loads `SymPy`, `ForwardDiff`\nusing Plots\n@vars x c real=true\n@vars h positive=true\nu = SymFunction(\"u\")\n\nex = divided_differences(u, c, c+h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a limit and see the familiar (yet differently represented) value of $u'(c)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "limit(ex, h => 0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ex = divided_differences(u, c, c+h, c+2h)\nsimplify(ex)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not so bad after simplification. The limit shows this to be an approximation to the second derivative divided by $2$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "limit(ex, h => 0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The expression is, up to a divisor of $2$, the second order forward\n[difference equation](http://tinyurl.com/n4235xy), a well-known\napproximation to $f''$.)\n\n\nThis relationship between higher-order divided differences and higher-order derivatives generalizes. This is expressed in this\n[theorem](http://tinyurl.com/zjogv83):\n\n> Suppose $m=x_0 < x_1 < x_2 < \\dots < x_n=M$ are distinct points. If $f$ has $n$\n> continuous derivatives then there exists a value $\\xi$ where $m < \\xi < M$ satisfying:\n\n$$~\nf[x_0, x_1, \\dots, x_n] = \\frac{1}{n!} \\cdot f^{(n)}(\\xi).\n~$$\n\nThis immediately applies to the above, where we parameterized by $h$:\n$x_0=c, x_1=c+h, x_2 = c+2h$. For then, as $h$ goes to $0$, it must be that $m, M\n\\rightarrow c$, and so the limit of the divided differences must\nconverge to $(1/2!) \\cdot f^{(2)}(c)$, as $f^{(2)}(\\xi)$ converges to $f^{(2)}(c)$.\n\nA proof based on Rolle's theorem appears in the appendix.\n\n\n## Quadratic approximations\n\nWhy the fuss? The answer comes from a result of Newton on\n*interpolating* polynomials. Consider a function $f$ and $n+1$ points\n$x_0$, $x_1, \\dots, x_n$. Then an interpolating polynomial is *the*\npolynomial of least degree that goes through each point $(x_i,\nf(x_i))$. The [Newton form](https://en.wikipedia.org/wiki/Newton_polynomial) of such a\npolynomial can be written as:\n\n$$~\nf[x_0] + f[x_0,x_1] \\cdot (x-x_0) + f[x_0, x_1, x_2] \\cdot (x-x_0) \\cdot (x-x_1) + \\cdots + f[x_0, x_1, \\dots, x_n] \\cdot (x-x_0)\\cdot \\cdots \\cdot (x-x_{n-1}).\n~$$\n\nThe case $n=0$ gives the value $f[x_0] = f(c)$, which can be interpreted as the slope-$0$ line that goes through the point $(c,f(c))$.\n\nWe are familiar with the case $n=1$, with $x_0=c$ and $x_1=c+h$, this becomes our secant-line formula:\n\n$$~\nf[c] + f[c, c+h](x-c).\n~$$\n\nAs mentioned, we can verify directly that it\ninterpolates the points $(c,f(c))$ and $(c+h, f(c+h))$. He we let `SymPy` do the algebra:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = divided_differences(u, c) + divided_differences(u, c, c+h) * (x-c)\np(x => c) - u(c)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "and"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p(x => c+h) - u(c+h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for something new. Take the $n=2$ case with\n$x_0 = c$, $x_1 = c + h$, and $x_2 = c+2h$. Then the interpolating polynomial is:\n\n$$~\nf[c] + f[c, c+h](x-c) + f[c, c+h, c+2h](x-c)(x-(c+h)).\n~$$\n\nWe add the next term to our previous polynomial and simplify"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = p + divided_differences(u, c, c+h, c+2h)*(x-c)*(x-(c+h))\nsimplify(p)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check that this interpolates the three points. Notice that at\n$x_0=c$ and $x_1=c+h$, the last term, $f[x_0, x_1,\nx_2]\\cdot(x-x_0)(x-x_1)$, vanishes, so we already have the polynomial\ninterpolating there. Only\nvalue $x_2=c+2h$ remains to be checked:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p(x => c+2h) - u(c+2h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, doesn't seem correct - that was supposed to be $0$. The issue isn't the math, it is that SymPy needs to be encouraged to simplify:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "simplify(p(x => c+2h) - u(c+2h))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "By contrast, at the point $x=c+3h$ we have no guarantee of interpolation, and indeed don't, as this expression is non-zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "simplify(p(x => c+3h) - u(c+3h))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolating polynomials are of interest in their own right,  but for now we want to use them as motivation for the best polynomial approximation of a certain degree for a function. Motivated by how the secant line leads to the tangent line, we note that coefficients of the quadratic interpolating polynomial above have limits as $h$ goes to $0$, leaving this polynomial:\n\n$$~\nf(c) + f'(c) \\cdot (x-c) + \\frac{1}{2!} \\cdot f''(c) (x-c)^2.\n~$$\n\nThis is clearly related to the tangent line approximation of $f(x)$ at\n$x=c$, but carrying an extra quadratic term.\n\nHere we visualize the approximations with\nthe function $f(x) = \\cos(x)$ at $c=0$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = cos(x)\na, b = -pi/2, pi/2\nc = 0\nh = 1/4\n\nfp = -sin(c)  # by hand, or use diff(f), ...\nfpp = -cos(c)\n\n\np = plot(f, a, b, linewidth=5, legend=false, color=:blue)\nplot!(p, x->f(c) + fp*(x-c), a, b; color=:green, alpha=0.25, linewidth=5)                     # tangent line is flat\nplot!(p, x->f(c) + fp*(x-c) + (1/2)*fpp*(x-c)^2, a, b; color=:green, alpha=0.25, linewidth=5)  # a parabola\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph illustrates that the extra quadratic term can track the\ncurvature of the function, whereas the tangent line itself can't. So,\nwe have a polynomial which is a \"better\" approximation, is it the best\napproximation?\n\n\nThe mean value theorem, as in the case of the tangent line, will guarantee the existence of $\\xi$ between $c$ and $x$, for which\n\n$$~\nf(x) - \\left(f(c) + f'(c) \\cdot(x-c) + (1/2)\\cdot f''(c) \\cdot (x-c)^2 \\right) =\n\\frac{1}{3!}f'''(\\xi) \\cdot (x-c)^3.\n~$$\n\nIn this sense, the above quadratic polynomial, called the Taylor Polynomial of degree 2, is the best *quadratic* approximation to $f$, as the difference goes to $0$.\n\n\nThe graphs of the secant line and approximating parabola for $h=1/4$ are similar:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0, x1, x2 = c-h, c, c+h\nf0 = divided_differences(f, x0)\nfd = divided_differences(f, x0, x1)\nfdd = divided_differences(f, x0, x1, x2)\n\nplot(f, a, b,                  color=:blue, linewidth=5, legend=false)\nplot!(x -> f0 + fd*(x-x0), a, b,     color=:green, alpha=0.25, linewidth=5);\nplot!(x -> f0 + fd*(x-x0) + fdd * (x-x0)*(x-x1), a,b,  color=:green, alpha=0.25, linewidth=5);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though similar, the graphs aren't identical, as the interpolating\npolynomials aren't the best approximations.  For example, in the\ntangent-line graph the parabola only intersects the cosine graph at\n$x=0$, whereas for the secant-line graph - by definition - the\nparabola intersects the graph at least $2$ times and the\ninterpolating polynomial $3$ times (at $x_0$, $x_1$, and $x_2$).\n\n\n\n\n##### Example\n\nConsider the function $f(t) = \\log(1 + t)$. We have mentioned that for $t$ small, the value $t$ is a good approximation. A better one becomes:\n\n$$~\nf(0) + f'(0) \\cdot t + \\frac{1}{2} \\cdot f''(0) \\cdot t^2 = 0 + 1t - \\frac{t^2}{2}\n~$$\n\nA graph shows the difference:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(t) = log(1 + t)\na, b = -1/2, 1\nplot(f, a, b, legend=false, linewidth=5)\nplot!(t -> t, a, b)\nplot!(t -> t - t^2/2, a, b)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though we can see that the tangent line is a good approximation, the\nquadratic polynomial tracks the logarithm better farther from $c=0$.\n\n##### Example\n\nA wire is bent in the form of a half circle with radius $R$ centered\nat $(0,R)$, so the bottom of the wire is at the origin. A bead is\nreleased on the wire at angle $\\theta$. As time evolves, the bead will\nslide back and forth. How? (Ignoring friction.)\n\n\nLet $U$ be the potential energy, $U=mgh = mgR \\cdot (1 -\n\\cos(\\theta))$. The velocity of the object will depend on $\\theta$ -\nit will be $0$ at the high point, and largest in magnitude at the\nbottom - and is given by $v(\\theta) = R \\cdot d\\theta/ dt$. (The bead\nmoves along the wire so its distance traveled is $R\\cdot \\Delta\n\\theta$, this, then, is just the time derivative of distance.)\n\nBy ignoring friction, the total energy is conserved giving:\n\n$$~\nK = \\frac{1}{2}m v^2 + mgR \\cdot (1 - \\cos(\\theta) =\n\\frac{1}{2} m R^2 (\\frac{d\\theta}{dt})^2 +  mgR \\cdot (1 - \\cos(\\theta)).\n~$$\n\nThe value of $1-\\cos(\\theta)$ inhibits further work which would be possible were there an easier formula there. In fact, we could try the excellent approximation $1 - \\theta^2/2$ from the quadratic approximation. Then we have:\n\n$$~\nK \\approx \\frac{1}{2} m R^2 (\\frac{d\\theta}{dt})^2 +  mgR \\cdot (1 - \\theta^2/2).\n~$$\n\nAssuming equality and differentiating in $t$ gives by the chain rule:\n\n$$~\n0 = \\frac{1}{2} m R^2 2\\frac{d\\theta}{dt} \\cdot \\frac{d^2\\theta}{dt^2} - mgR \\theta\\cdot \\frac{d\\theta}{dt}.\n~$$\n\nThis can be solved to give this relationship:\n\n$$~\n\\frac{d^2\\theta}{dt^2} = - \\frac{g}{R}\\theta.\n~$$\n\nThe solution to this \"equation\" can be written (in some\nparameterization) as $\\theta(t)=A\\cos \\left(\\omega t+\\phi\n\\right)$. This motion is the well-studied simple [harmonic\noscillator](https://en.wikipedia.org/wiki/Harmonic_oscillator), a\nmodel for a simple pendulum.\n\n## The Taylor polynomial of degree $n$\n\n\nStarting with the Newton form of the interpolating polynomial of smallest degree:\n\n$$~\nf[x_0] + f[x_0,x_1] \\cdot (x - x_0) + f[x_0, x_1, x_2] \\cdot (x - x_0)\\cdot(x-x_1) +\n\\cdots + f[x_0, x_1, \\dots, x_n] \\cdot (x-x_0) \\cdot \\cdots \\cdot (x-x_{n-1}).\n~$$\n\nand taking $x_i = c + i\\cdot h$, for a given $n$, we have in the limit as $h > 0$ goes to zero that coefficients of this polynomial converge to the coefficients of the *Taylor Polynomial of degree n*:\n\n$$~\nf(c) + f'(c)\\cdot(x-c) + \\frac{f''(c)}{2!}(x-c)^2 + \\cdots + \\frac{f^{(n)}(c)}{n!} (x-c)^n\n~$$\n\n\n\nThis polynomial will be a good approximation to the function $f$, near\n$c$. The error will be given - again by an application of the Mean\nValue Theorem - by $(1/(n+1)!) \\cdot f^{(n+1)}(\\xi) \\cdot (x-c)^n$ for some $\\xi$ between $c$ and $x$.\n\n\n\nThe Taylor polynomial for $f$ about $c$ of degree $n$ can be computed\nby taking $n$ derivatives. For such a task, the computer is very\nhelpful. In `SymPy` the `series` function will compute the Taylor\npolynomial for a given $n$. For example, here is the series expansion\nto 10 terms of the function $\\log(1+x)$ about $c=0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x\nc, n = 0, 10\nl = series(log(1 + x), x, c, n+1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pattern can be observed.\n\n\n\nUsing `series`, we can see Taylor polynomials for several familiar functions:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "series(1/(1-x), x, 0, 10)   # sum x^i for i in 0:n"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "series(exp(x), x, 0, 10)    # sum x^i/i! for i in 0:n"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "series(sin(x), x, 0, 10)    # sum (-1)^i * x^(2i+1) / (2i+1)! for i in 0:n"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "series(cos(x), x, 0, 10)    # sum (-1)^i * x^(2i) / (2i)! for i in 0:n"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these last three have a pattern that can be expressed quite succinctly if the denominator is recognized as $n!$.\n\n\nThe output of `series` includes a big \"Oh\" term, which identifies the\nscale of the error, but also gets in the way of using the\noutput. `SymPy` provides the `removeO` method to strip this. (It is called as `object.removeO()`, as it is a method of an object in SymPy.)\n\n\nHowever, we will define our own function to compute Taylor polynomials from a function. The following returns a function, not a symbolic object, using `D`, from `CalculusWithJulia`, which is based on `ForwardDiff.derivative`, to find higher-order derivatives:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function taylor_poly(f, c=0, n=2)\n     x -> f(c) + sum(D(f, i)(c) * (x-c)^i / factorial(i) for i in 1:n)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a function, we can compare values.\nFor example, here we see the difference between the Taylor polynomial and the answer for a small value of $x$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "a = .1\nf(x) = log(1+x)\nTn = taylor_poly(f, 0, 5)\nTn(a) - f(a)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting\n\nLet's now visualize a function and the two approximations - the Taylor\npolynomial and the interpolating polynomial. We use this function to\ngenerate the interpolating polynomial as a function:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function newton_form(f, xs)\n  x -> begin\n     tot = divided_differences(f, xs[1])\n     for i in 2:length(xs)\n        tot += divided_differences(f, xs[1:i]...) * prod([x-xs[j] for j in 1:(i-1)])\n     end\n     tot\n  end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see a plot, we have"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = sin(x)\nc, h, n = 0, 1/4, 4\nint_poly = newton_form(f, [c + i*h for i in 0:n])\ntp = taylor_poly(f, c, n)\na, b = -pi, pi\nplot(sin, a, b; linewidth=5)\nplot!(int_poly, a, b, color=:green)\nplot!(tp, a, b, color=:red)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better sense, we plot the residual differences here:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "d1(x) = f(x) - int_poly(x)\nd2(x) = f(x) - tp(x)\na, b = -pi, pi\nplot(d1, a, b, color=:blue)\nplot!(d2, a, b, color=:green)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph should be $0$ at each of the the points in `xs`, which we\ncan verify in the graph above. Plotting over a wider region shows a\ncommon phenomenon that these polynomials approximate the function near\nthe values, but quickly deviate away:\n\n\nIn this graph we make a plot of the Taylor polynomial for different sizes of $n$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = 1 - cos(x)\na, b = -pi, pi\nplot(f, a, b, linewidth=5)\nplot!(taylor_poly(f, 0, 2), a, b)\nplot!(taylor_poly(f, 0, 4), a, b)\nplot!(taylor_poly(f, 0, 6), a, b)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though all are good approximations near $c=0$, as more terms are\nincluded, the Taylor polynomial becomes a better approximation over a wider\nrange of values.\n\n\n##### Example: Period of an orbiting satellite\n\nKepler's third [law](http://tinyurl.com/y7oa4x2g) of planetary motion states:\n\n> The square of the orbital period of a planet is directly proportional to the cube of the semi-major axis of its orbit.\n\nIn formulas, $P^2 = a^3 \\cdot (4\\pi^2) / (G\\cdot(M + m))$, where $M$ and $m$ are the respective masses. Suppose a satellite is in low earth orbit with a constant height, $a$. Use a Taylor polynomial to approximate the period using Kepler's third law to relate the quantities.\n\nSuppose $R$ is the radius of the earth and $h$ the height above the earth assuming $h$ is much smaller than $R$. The mass $m$ of a satellite is negligible to that of the earth, so $M+m=M$ for this purpose. We have:\n\n$$~\nP = \\frac{2\\pi}{\\sqrt{G\\cdot M}} \\cdot (h+R)^{3/2} =  \\frac{2\\pi}{\\sqrt{G\\cdot M}} \\cdot R^{3/2} \\cdot (1 + h/R)^{3/2} = P_0 \\cdot  (1 + h/R)^{3/2},\n~$$\n\nwhere $P_0$ collects terms that involve the constants.\n\nWe can expand $(1+x)^{3/2}$ to fifth order, to get:\n\n$$~\n(1+x)^{3/2} \\approx 1 + \\frac{3x}{2} + \\frac{3x^2}{8} - \\frac{1x^3}{16} + \\frac{3x^4}{128} -\\frac{3x^5}{256}\n~$$\n\nOur approximation becomes:\n\n$$~\nP \\approx P_0 \\cdot (1 + \\frac{3(h/R)}{2} + \\frac{3(h/R)^2}{8} - \\frac{(h/R)^3}{16} + \\frac{3(h/R)^4}{128} - \\frac{3(h/R)^5}{256}).\n~$$\n\nTypically, if $h$ is much smaller than $R$ the first term is enough giving a formula like $P \\approx P_0 \\cdot(1 + \\frac{3h}{2R})$.\n\n\nA satellite phone utilizes low orbit satellites to relay phone communications. The [Iridium](http://www.kddi.com/english/business/cloud-network-voice/satellite/iridium/mobile/) system uses satellites with an elevation $h=780km$. The radius of the earth is $3,959$ miles, the mass of the earth is $5.972 × 10^{24} kg$, and the gravitational [constant](https://en.wikipedia.org/wiki/Gravitational_constant), $G$ is $6.67408 \\cdot 10^{-11}$ $m^3/(kg \\cdot s^2)$.\n\nCompare the approximate value with 1 term to the exact value."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "G = 6.67408e-11\nh = 780 * 1000\nR =  3959 * 1609.34   # 1609 meters per mile\nM = 5.972e24\nP0, hR = (2pi)/sqrt(G*M) * R^(3/2), h/R\n\nPreal = P0 * (1 + hR)^(3/2)\nP1 = P0 * (1 + 3*hR/2)\nPreal, P1"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With terms out to the fifth power, we get a better approximation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "P5 = P0 * (1 + 3*hR/2 + 3*hR^2/8 - hR^3/16 + 3*hR^4/128 - 3*hR^5/256)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The units of the period above are in seconds. That answer here is about 100 minutes:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Preal/60"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $h$ is much smaller than $R$ the approximation with 5th order is\nreally good, and serviceable with just 1 term. Next we check if this\nis the same when $h$ is larger than $R$.\n\n----\n\nThe height of a [GPS satellite](http://www.gps.gov/systems/gps/space/) is about $12,550$ miles. Compute the period of a circular orbit and compare with the estimates."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "h = 12250 * 1609.34   # 1609 meters per mile\nhR = h/R\n\nPreal = P0 * (1 + hR)^(3/2)\nP1 = P0 * (1 + 3*hR/2)\nP5 = P0 * (1 + 3*hR/2 + 3*hR^2/8 - hR^3/16 + 3*hR^4/128 - 3*hR^5/256)\n\nPreal, P1, P5"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the Taylor polynomial underestimates badly in this case. A reminder\nthat these approximations are locally good, but may not be good on all\nscales. Here $h \\approx 3R$. We can see from this graph\nof $(1+x)^{3/2}$ and its 5th degree Taylor polynomial $T_5$ that it is a bad approximation when $x > 2$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f1(x) = (1+x)^(3/2)\np2(x) = 1 + 3x/2 + 3x^2/8 - x^3/16 + 3x^4/128 - 3x^5/256\nplot(f1, -1, 3, linewidth=4, legend=false)\nplot!(p2, -1, 3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n\nFinally, we show how to use the `Unitful` package to work with the\nunits. This package allows us to define different units, carry these\nunits through computations, and convert between similar units with\n`uconvert`. In this example, we define several units, then show how\nthey can then be used as constants."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Unitful\nm, mi, kg, s, hr = u\"m\", u\"mi\", u\"kg\", u\"s\", u\"hr\"\n\nG = 6.67408e-11 * m^3 / kg / s^2\nh = uconvert(m, 12250 * mi)   # unit convert miles to meter\nR = uconvert(m,  3959 * mi)\nM = 5.972e24 * kg\n\nP0, hR = (2pi)/sqrt(G*M) * R^(3/2), h/R\nPreal = P0 * (1 + hR)^(3/2)    # in seconds"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see `Preal` has the right units - the units of mass and distance cancel leaving a measure of time - but it is hard to sense how long this is. Converting to hours, helps us see the satellite orbits about twice per day:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "uconvert(hr, Preal)  # ≈ 11.65 hours"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example Computing $\\log(x)$\n\nWhere exactly does the value assigned to $\\log(5)$ come from? The\nvalue needs to be computed. At some level, many questions resolve down\nto the basic operations of addition, subtraction, multiplication, and\ndivision. Preferably not the latter, as division is slow. Polynomials\nthen should be fast to compute, and so computing logarithms using a\npolynomial becomes desirable.\n\nBut how? One can see details of a possible\nway\n[here](https://github.com/musm/Amal.jl/blob/master/src/log.jl).\n\nFirst, there is usually a reduction stage. In this phase, the problem\nis transformed in a manner to one involving only a fixed interval of values. For this\nfunction values of $k$ and $m$ are found so that $x = 2^k \\cdot (1+m)$\n*and* $\\sqrt{2}/2 < 1+m < \\sqrt{2}$. If these are found, then $\\log(x)$ can be computed with\n$k \\cdot \\log(2) + \\log(1+m)$. The first value - a multiplication - can easily be\ncomputed using  pre-computed value of $\\log(2)$, the second then *reduces* the problem to an interval.\n\n\nNow, for this problem a further\ntrick is utilized, writing $s= f/(2+f)$ so that\n$\\log(1+m)=\\log(1+s)-\\log(1-s)$.  $\\log(1+s) - \\log(1-s)$ for some small range of $s$ values then makes it possible to compute $\\log(x)$ for any real $x$.\n\nTo compute $\\log(1\\pm s)$, we can find a Taylor series. Let's go out to degree $19$ and use `SymPy` to do the work:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars s\na = series(log(1 + s), s, 0, 19)\nb = series(log(1 - s), s, 0, 19)\na_b = (a - b).removeO()  # remove\"Oh\" not remove\"zero\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is re-expressed as $2s + s \\cdot p$ with $p$ given by:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = cancel(a_b - 2s/s)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, $2s = m - s\\cdot m$, so the above can be reworked to be $\\log(1+m) = m - s\\cdot(m-p)$.\n\n\n(For larger values of $m$, a similar, but different approximation, can be used to minimize floating point errors.)\n\n\nHow big can the error be between this *approximations* and $\\log(1+m)$? We plot to see how big $s$ can be:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars u\nplot(u/(2+u), sqrt(2)/2 - 1, sqrt(2)-1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows, $s$ is as big as"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "M = (u/(2+u))(u => sqrt(2) - 1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error term is like $2/19 \\cdot \\xi^{19}$ which  is largest at this value of $M$. Large is relative - it is really small:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "(2/19)*M^19"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically that is machine precision. Which means, that as far as can be told on the computer, the value produced by $2s + s \\cdot p$ is as accurate as can be done.\n\nTo try this out to compute $\\log(5)$. We have $5 = 2^2(1+0.25)$, so $k=2$ and $m=0.25$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "k, m = 2, 0.25\ns = m / (2+m)\np = 2 * sum(s^(2i)/(2i+1) for i in 1:8)  # where the polynomial approximates the logarithm...\n\nlog(1 + m), m - s*(m-p), log(1 + m) - ( m - s*(m-p))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two values differ by less than $10^{-16}$ as advertised. Re-assembling then, we compare the computed values:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "k * log(2) + (m - s*(m-p)), log(5)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual  code is different, as the Taylor polynomial isn't\nused. The Taylor polynomial is a great approximation near a point, but\nthere might be better approximations for all values in an interval.\nIn this case there is, and that is used in the production\nsetting. This makes things a bit more efficient, but the basic idea\nremains - for a prescribed accuracy, a polynomial approximation can\nbe found over a given interval, which can be cleverly utilized to\nsolve for all applicable values.\n\n\n\n\n\n\n## Questions\n\n###### Question\n\nCompute the Taylor polynomial  of degree 10 for $\\sin(x)$ about $c=0$ using `SymPy`. Based on the form, which formula seems appropriate:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"\\sum_{k=0}^{10} x^k\",\nL\"\\sum_{k=1}^{10} (-1)^{n+1} x^n/n\",\nL\"\\sum_{k=0}^{4} (-1)^k/(2k+1)! \\cdot x^{2k+1}\",\nL\"\\sum_{k=0}^{10} x^n/n!\"\n]\nans = 3\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nCompute the Taylor polynomial  of degree 10 for $e^x$ about $c=0$ using `SymPy`. Based on the form, which formula seems appropriate:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"\\sum_{k=0}^{10} x^k\",\nL\"\\sum_{k=1}^{10} (-1)^{n+1} x^n/n\",\nL\"\\sum_{k=0}^{4} (-1)^k/(2k+1)! \\cdot x^{2k+1}\",\nL\"\\sum_{k=0}^{10} x^n/n!\"\n]\nans = 4\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nCompute the Taylor polynomial  of degree 10 for $1/(1-x)$ about $c=0$ using `SymPy`. Based on the form, which formula seems appropriate:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"\\sum_{k=0}^{10} x^k\",\nL\"\\sum_{k=1}^{10} (-1)^{n+1} x^n/n\",\nL\"\\sum_{k=0}^{4} (-1)^k/(2k+1)! \\cdot x^{2k+1}\",\nL\"\\sum_{k=0}^{10} x^n/n!\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLet $T_5(x)$ be the Taylor polynomial of degree 5 for the function $\\sqrt{1+x}$ about $x=0$. What is the coefficient of the $x^5$ term?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"7/256\",\nL\"-5/128\",\nL\"1/5!\",\nL\"2/15\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nThe 5th order Taylor polynomial for $\\sin(x)$ about $c=0$ is: $x - x^3/3! + x^5/5!$. Use this to find the first 3 terms of the Taylor polynomial of $\\sin(x^2)$ about $c=0$.\n\nThey are:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"x^2 - x^6/3! + x^{10}/5!\",\nL\"x^2\",\nL\"x^2 \\cdot (x - x^3/3! + x^5/5!)\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nA more direct derivation of the form of the Taylor polynomial (here taken about $c=0$) is to *assume* a polynomial form that matches $f$:\n\n$$~\nf(x) = a + bx + cx^2 + dx^3 + ex^4 + \\cdots\n~$$\n\nIf this is true, then formally evaluating at $x=0$ gives $f(0) = a$, so $a$ is determined. Similarly, formally differentiating and evaluating at $0$ gives $f'(0) = b$. What is the result of formally differentiating $4$ times and evaluating at $0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [L\"f''''(0) = e\",\nL\"f''''(0) = 4\\cdot 3\\cdot2 e\",\nL\"f''''(0) = 0\"]\nans = 2\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nHow big an error is there in approximating $e^x$ by its 5th degree Taylor polynomial about $c=0$, $1 + x + x^2/2! + x^3/3! + x^4/4! + x^5/5!$?, over $[-1,1]$.\n\nThe error is known to be $(  f^{(6)}(\\xi)/6!) \\cdot x^6$ for some $\\xi$ in $[-1,1]$.\n\n\n* The 6th derivative of $e^x$ is still $e^x$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Which is true about the function $e^x$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices =[\"It is increasing\", \"It is decreasing\", \"It both increases and decreases\"]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The maximum value of $e^x$ over $[-1,1]$ occurs at"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices=[\"A critical point\", \"An end point\"]\nans = 2\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Which theorem tells you that for a *continuous* function over  *closed* interval, a maximum value will exist?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"The intermediate value theorem\",\n\"The mean value theorem\",\n\"The extreme value theorem\"]\nans = 3\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What is the *largest* possible value of the error:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"1/6!\\cdot e^1 \\cdot 1^6\",\nL\"1^6 \\cdot 1 \\cdot 1^6\"]\nans = 1\nradioq(choices,ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nThe error in using $T_k(x)$ to approximate $e^x$ over the interval $[-1/2, 1/2]$ is $(1/(k+1)!) e^\\xi x^{k+1}$, for some $\\xi$ in the interval. This is *less* than $1/((k+1)!) e^{1/2} (1/2)^{k+1}$.\n\n* Why?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"The function $e^x$ is increasing, so takes on its largest value at the endpoint and the function $|x^n| \\leq |x|^n \\leq (1/2)^n$\",\nL\"The function has a critical point at $x=1/2$\",\nL\"The function is monotonic in $k$, so achieves its maximum at $k+1$\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming the above is right, find the smallest value $k$ guaranteeing a error no more than $10^{-16}$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(k) = 1/factorial(k+1) * exp(1/2) * (1/2)^(k+1)\n(f(13) > 1e-16 && f(14) < 1e-16) && numericq(14)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The function $f(x) = (1 - x + x^2) \\cdot e^x$ has a Taylor polynomial about 0 such that all coefficients are rational numbers. Is it true that the numerators are all either 1 or prime? (From the 2014 [Putnam](http://kskedlaya.org/putnam-archive/2014.pdf) exam.)\n\nHere is one way to get all the values bigger than 1:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x\nex = (1 - x + x^2)*exp(x)\nTn = series(ex, x, 0, 100).removeO()\nps = sympy.Poly(Tn, x).coeffs()\nqs = numer.(ps)\nqs[qs .> 1]  |> Tuple # format better for output"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify by hand that each of the remaining values is a prime number to answer the question (Or you can use `sympy.isprime.(qs)`).\n\nAre they all prime or $1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix\n\nWe mentioned two facts that could use a proof: the Newton form of the interpolating polynomial and the mean value theorem for divided differences. Our explanation tries to emphasize a parallel with the secant line's relationship with the tangent line. The standard way to discuss the Taylor polynomial is different and so these two proofs are not in most calculus texts.\n\nA [proof](https://www.math.uh.edu/~jingqiu/math4364/interpolation.pdf) of the Newton form can be done knowing that the interpolating polynomial is unique and can be expressed either as $g(x)=a_0 + a_1 (x-x_0) + \\cdots + a_n (x-x_0)\\cdot\\cdots\\cdot(x-x_{n-1})$ *or* in this reversed form\n$h(x)=b_0 + b_1 (x-x_n) + b_2(x-x_n)(x-x_{n-1}) + \\cdots + b_n (x-x_n)(x-x_{n-1})\\cdot\\cdots\\cdot(x-x_1)$. These two polynomials are of degree $n$ at most and have $u(x) = h(x)-g(x)=0$, by uniqueness. So the coefficients of $u(x)$ are $0$. We have that the coefficient of $x^n$ must be $a_n-b_n$ so $a_n=b_n$. Our goal is to express $a_n$ in terms of $a_{n-1}$ and $b_{n-1}$. Focusing on the $x^{n-1}$ term, we have:\n\n$$~\nb_n(x-x_n)(x-x_{n-1})\\cdot\\cdots\\cdot(x-x_1) - a_n\\cdot(x-x_0)\\cdot\\cdots\\cdot(x-x_{n-1})=\na_n [(x-x_1)\\cdot\\cdots\\cdot(x-x_{n-1})] [(x- x_n)-(x-x_0)] =\n-a_n \\cdot(x_n - x_0) x^{n-1} + p_{n-2},\n~$$\n\nwhere $p_{n-2}$ is a polynomial of at most degree $n-2$. (The expansion of $(x-x_1)\\cdot\\cdots\\cdot(x-x_{n-1}))$ leaves $x^{n-1}$ plus some lower degree polynomial.) Similarly, we have\n$a_{n-1}(x-x_0)\\cdot\\cdots\\cdot(x-x_{n-2}) = a_{n-1}x^{n-1} + q_{n-2}$ and\n$b_{n-1}(x-x_n)\\cdot\\cdots\\cdot(x-x_2) = b_{n-1}x^{n-1}+r_{n-2}$. Combining, we get that the $x^{n-1}$ term of $u(x)$ is\n\n$$~\n(b_{n-1}-a_{n-1}) - a_n(x_n-x_0) = 0.\n~$$\n\nOn rearranging, this yields $a_n = (b_{n-1}-a_{n-1}) / (x_n - x_0)$. By *induction* - that $a_i=f[x_0, x_1, \\dots, x_i]$ and $b_i = f[x_n, x_{n-1}, \\dots, x_{n-i}]$ (which has trivial base case) - this is $(f[x_1, \\dots, x_n] - f[x_0,\\dots x_{n-1}])/(x_n-x_0)$.\n\nNow, assuming the Newton form is correct, a\n[proof](http://tinyurl.com/zjogv83) of the mean value theorem for\ndivided differences comes down to Rolle's theorem. Starting from the\nNewton form of the polynomial and expanding in terms of\n$1, x, \\dots, x^n$ we see that\n$g(x) = p_{n-1}(x) + f[x_0, x_1, \\dots,x_n]\\cdot x^n$,\nwhere now $p_{n-1}(x)$ is a\npolynomial of degree  at most $n-1$. That is, the coefficient of\n$x^n$ is $f[x_0, x_1, \\dots, x_n]$. Consider  the function $h(x)=f(x) - g(x)$.\nIt has zeros $x_0, x_1, \\dots, x_n$.\n\nBy Rolle's theorem, between any two such zeros $x_i, x_{i+1}$, $0 \\leq i < n$ there must be a zero of the derivative of $h(x)$, say $\\xi^1_i$. So $h'(x)$ has zeros $\\xi^1_0 < \\xi^1_1 < \\dots < \\xi^1_{n-1}$.\n\n\nWe visualize this with $f(x) = \\sin(x)$ and $x_i = i$ for $i=0, 1, 2, 3$, The $x_i$ values are indicated with circles, the $\\xi^1_i$ values indicated with squares:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Roots\nf(x) = sin(x)\nxs = 0:3\ndd = divided_differences\ng(x) = dd(f,0) + dd(f, 0,1)*x + dd(f, 0,1,2)*x*(x-1) + dd(f, 0,1,2,3)*x*(x-1)*(x-2)\nh1(x) = f(x) - g(x)\ncps = find_zeros(D(h1), -1, 4)\nplot(h1, -1/4, 3.25, linewidth=3, legend=false)\nscatter!(xs, h1.(xs))\nscatter!(cps, h1.(cps), markersize=3, marker=:square)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again by Rolle's theorem, between any pair of adjacent zeros $\\xi^1_i, \\xi^1_{i+1}$ there must be a zero $\\xi^2_i$ of $h''(x)$. So there are $n-1$ zeros of $h''(x)$. Continuing, we see that there will be\n$n+1-3$ zeros of $h^{(3)}(x)$,\n$n+1-4$ zeros of $h^{4}(x)$, $\\dots$,\n$n+1-(n-1)$ zeros of $h^{n-1}(x)$, and finally\n$n+1-n$ ($1$) zero of $h^{(n)}(x)$. Call this last zero $\\xi$. It satisfies $x_0 \\leq \\xi \\leq x_n$. Further,\n$0 = h^{(n)}(\\xi) = f^{(n)}(\\xi)  - g^{(n)}(\\xi)$. But $g$ is a degree $n$ polynomial, so the $n$th derivative is the coefficient of $x^n$ times $n!$. In this case we have $0 = f^{(n)}(\\xi) - f[x_0, \\dots, x_n] n!$. Rearranging yields the result."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.0"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
