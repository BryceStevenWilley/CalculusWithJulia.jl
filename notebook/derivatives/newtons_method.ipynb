{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's method"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia\nusing CalculusWithJulia.WeaveSupport\nusing Plots\nnothing"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Babylonian method is an algorithm to find an approximate value for $\\sqrt{k}$.\nIt was described by the first-century Greek mathematician Hero of\n[Alexandria](http://en.wikipedia.org/wiki/Babylonian_method).\n\nThe method starts with some initial guess, called $x_0$. It then\napplies a formula to produce an improved guess. This is repeated until\nthe improved guess is accurate enough or it is clear the algorithm\nfails to work.\n\nFor the Babylonian method, the next guess, $x_{i+1}$ derived from the current guess, $x_i$, is:\n\n\n$$~\nx_{i+1} = \\frac{1}{2}(x_i + \\frac{k}{x_i})\n~$$\n\n\nWe use this algorithm to approximate the square root of $2$, a value known to\nthe Babylonians.\n\nStart with $x$, then form $x/2 + 1/x$, from this again form $x/2 + 1/x$, repeat.\n\nLet's look starting with $x = 2$ as a  rational number:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 2//1\nx = x//2 + 1//x\nx, x^2.0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our estimate improved from something which squared to $4$ down to something which squares to $2.25$. A big improvement, but there is still more to come."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = x//2 + 1//x\nx, x^2.0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now see accuracy until the third decimal point."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = x//2 + 1//x\nx, x^2.0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is now accurate to the sixth decimal point.  That is about as far\nas we, or the Bablyonians, would want to go by hand. Using rational\nnumbers quickly grows out of hand. The next step shows the explosion:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = x//2 + 1//x"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, with the advent of floating point numbers, the method stays quite manageable:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 2.0\nx = x/2 + 1/x   # 1.5, 2.25\nx = x/2 + 1/x   # 1.4166666666666665, 2.006944444444444\nx = x/2 + 1/x   # 1.4142156862745097, 2.0000060073048824\nx = x/2 + 1/x   # 1.4142135623746899, 2.0000000000045106\nx = x/2 + 1/x   # 1.414213562373095,  1.9999999999999996\nx = x/2 + 1/x   # 1.414213562373095,  1.9999999999999996"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the algorithm - to the precision offered by floating\npoint numbers - has resulted in an answer `1.414213562373095`. This\nanswer is an *approximation* to the actual answer. Approximation is necessary,\nas $\\sqrt{2}$ is an irrational number and so can never be exactly\nrepresented in floating point. That being said, we see that the value\nof $f(x)$ is accurate to the last decimal place, so our approximation\nis very close and is achieved in a few steps.\n\n## Newton's generalization\n\nLet $f(x) = x^3 - 2x -5$. The value of 2 is almost a zero, but not quite, as $f(2) =\n-1$. We can check that there are no *rational* roots. Though there is\na method to solve the cubic it may be difficult to compute and will\nnot be as generally applicable as some algorithm like the Babylonian\nmethod to produce an approximate answer.\n\nIs there some generalization to the Babylonian method?\n\nWe know that the tangent line is a good approximation to the function\nat the point. Looking at this graph gives a hint as to an algorithm:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nf(x) = x^3 - 2x - 5\nfp(x) = 3x^2 - 2\nc = 2\np = plot(f, 1.75, 2.25, legend=false)\nplot!(x->f(2) + fp(2)*(x-2))\nplot!(zero)\nscatter!(p, [c], [f(c)], color=:orange, markersize=3)\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tangent line and the function nearly agree near $2$. So much so,\nthat the intersection point of the tangent line with the $x$ axis\nnearly hides the actual zero of $f(x)$ that is near $2.1$.\n\nThat is, it seems that the intersection of the tangent line and the\n$x$ axis should be an improved approximation for the zero of the\nfunction.\n\nLet $x_0$ be $2$, and $x_1$ be the intersection point of the tangent line\nat $(x_0, f(x_0))$ with the $x$ axis. Then by the definition of the\ntangent line:\n\n$$~\nf'(x_0) = \\frac{\\Delta y }{\\Delta x} = \\frac{f(x_0)}{x_0 - x_1}.\n~$$\n\nThis can be solved for $x_1$ to give $x_1 = x_0 - f(x_0)/f'(x_0)$. In general, if we had $x_i$ and used the intersection point of the tangent line to produce $x_{i+1}$ we would have Newton's method:\n\n$$~\nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}.\n~$$\n\n\nWe will use automatic derivatives, as possible, so load the `CalculusWithJulia` package which provides the `f'` notation for derivatives through the definition `Base.adjoint(f::Function)=x->ForwardDiff.derivative(f, float(x))`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CalculusWithJulia\nusing Plots"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, the algorithm above starting from $2$ becomes:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x0 = 2\nx1 = x0 - f(x0)/f'(x0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we are closer to a zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x0), f(x1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying again, we have"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x2 = x1 - f(x1)/ f'(x1)\nx2, f(x2), f(x1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And again:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x3 = x2 - f(x2)/ f'(x2)\nx3, f(x3), f(x2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x4 = x3 - f(x3)/ f'(x3)\nx4, f(x4), f(x3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see now that $f(x_4)$ is within machine tolerance of $0$, so we\ncall $x_4$ an *approximate zero* of $f(x)$.\n\n\n> Newton's method. Let $x_0$ be an initial guess for a zero of\n> $f(x)$. Iteratively define $x_{i+1}$ in terms of the just\n> generated $x_i$ by: $x_{i+1} = x_i - f(x_i) / f'(x_i)$. Then for\n> most functions and reasonable initial guesses, the sequence of\n> points converges to a zero of $f$.\n\nOn the computer, we know that actual convergence will likely never\noccur, but accuracy to a certain tolerance can often be achieved.\n\n\n\nIn the example above, we kept track of the previous values. This is\nunnecessary if only the answer is sought. In that case, the update\nstep can use the same variable:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 2                     # x0\nx = x - f(x) / f'(x)      # x1\nx = x - f(x) / f'(x)      # x2\nx = x - f(x) / f'(x)      # x3\nx = x - f(x) / f'(x)      # x4"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, the assignment will update the value bound to `x` using the previous value of `x` in the computation.\n\nWe implement the algorithm by repeating the step until either we\nconverge or it is clear we won't converge. For good guesses and most\nfunctions, convergence happens quickly."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "note(\"\"\"\n\nNewton looked at this same example in 1699 (B.T. Polyak, *Newton's\nmethod and its use in optimization*, European Journal of Operational\nResearch. 02/2007; 181(3):1086-1096.) though his technique was\nslightly different as he did not use the derivative, *per se*, but\nrather an approximation based on the fact that his function was a\npolynomial (though identical to the derivative). Raphson (1690)\nproposed the general form, hence the usual name of the Newton-Raphson\nmethod.\n\n\n\"\"\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples\n\n##### Example: visualizing convergence\n\nThis  graphic demonstrates the method and the rapid convergence:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### {{{newtons_method_example}}}\n\npyplot()\nfig_size = (600, 400)\n\n\nfunction newtons_method_graph(n, f, a, b, c)\n\n    xstars = [c]\n    xs = [c]\n    ys = [0.0]\n\n    plt = plot(f, a, b, legend=false, size=fig_size)\n    plot!(plt, [a, b], [0,0], color=:black)\n\n\n    ts = range(a, stop=b, length=50)\n    for i in 1:n\n        x0 = xs[end]\n        x1 = x0 - f(x0)/D(f)(x0)\n        push!(xstars, x1)\n            append!(xs, [x0, x1])\n        append!(ys, [f(x0), 0])\n    end\n    plot!(plt, xs, ys, color=:orange)\n    scatter!(plt, xstars, 0*xstars, color=:orange, markersize=5)\n    plt\nend\ncaption = \"\"\"\n\nIllustration of Newton's Method converging to a zero of a function.\n\n\"\"\"\nn = 6\n\nfn, a, b, c = x->log(x), .15, 2, .2\n\nanim = @animate for i=1:n\n    newtons_method_graph(i-1, fn, a, b, c)\nend\n\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 1)\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example: numeric not algebraic\n\nFor the function $f(x) = \\cos(x) - x$, we see that SymPy can not solve symbolically for a zero:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@vars x real=true\nsolve(cos(x) - x, x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find a numeric solution, even though there is no closed-form answer. Here we try Newton's method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = cos(x) - x\nx = .5\nx = x - f(x)/f'(x)  # 0.7552224171056364\nx = x - f(x)/f'(x)  # 0.7391416661498792\nx = x - f(x)/f'(x)  # 0.7390851339208068\nx = x - f(x)/f'(x)  # 0.7390851332151607\nx = x - f(x)/f'(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This answer is close, to machine tolerance it produces $0.0$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x, f(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example division as multiplication\n\n[Newton-Raphson Division](http://tinyurl.com/kjj9w92) is a means to divide by multiplying.\n\nWhy would you want to do that? Well, even for computers division is\nharder (read slower) than multiplying. The trick is that $p/q$ is\nsimply $p \\cdot (1/q)$, so finding a means to compute a reciprocal by\nmultiplying will reduce division to multiplication.  (This trick is\nused by\n[yeppp](http://www.yeppp.info/resources/ppam-presentation.pdf), a high\nperformance library for computational mathematics.)\n\n\nWell suppose we have $q$, we could try to use Newton's method to find\n$1/q$, as it is a solution to $f(x) = x - 1/q$. The Newton update step\nsimplifies to:\n\n$$~\nx - f(x) / f'(x) \\quad\\text{or}\\quad x - (x - 1/q)/ 1 = 1/q\n~$$\n\nThat doesn't really help, as Newton's method is just $x_{i+1} = 1/q$\n- that is it just jumps to the answer, the one we want to compute by\nsome other means!\n\n\nTrying again, we simplify the update step for a related function:\n$f(x) = 1/x - q$ with $f'(x) = -1/x^2$ and then one step of the process is:\n\n$$~\nx_{i+1} = x_i - (1/x_i - q)/(-1/x_i^2) = -qx^2_i + 2x_i.\n~$$\n\nNow for $q$ in the interval $[1/2, 1]$ we want to get a *good* initial\nguess. Here is a claim. We can use $x_0=48/17 - 32/17 \\cdot q$. Let's check\ngraphically that this is a reasonable initial approximation to $1/q$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "g(q) = 1/q\nh(q) = 1/17 * (48 - 32q)\nplot(g, 1/2, 1)\nplot!(h)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be shown that we have for any $q$ in $[1/2, 1]$ with initial guess $x_0 =\n48/17 - 32/17\\cdot q$ that Newton's method will converge to 16 digits in no more\nthan this many steps:\n\n$$~\n\\log_2(\\frac{53 + 1}{\\log_2(17)}).\n~$$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "a = log2((53 + 1)/log2(17))\nceil(Integer, a)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is 4 steps suffices.\n\nFor $q = 0.80$, to find $1/q$ using the above we have"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q = 0.80\nx = (48/17) - (32/17)*q\nx = -q*x*x + 2*x\nx = -q*x*x + 2*x\nx = -q*x*x + 2*x\nx = -q*x*x + 2*x"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method has basically $18$ multiplication and addition operations\nfor one division, so it naively would seem slower, but timing this\nshows the method is competitive with a regular division.\n\n## A function\n\nIn the previous example, a bound ensures convergence in 4 steps. In\ngeneral, this is not the case with Newton's method where the algorithm\nis iterated until convergence.  Having to repeat steps until something\nhappens is a task best done by the computer. The `while` loop is a\ngood way to repeat commands until some condition is met. With this, we\npresent a simple function implementing Newton's method, we iterate\nuntil the update step gets really small (the `delta`) or the\nconvergence takes more than 50 steps. (There are other reasonable choices that could be used to determine when the algorithm should stop.)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function nm(f, fp, x0)\n  tol = 1e-14\n  ctr = 0\n  delta = Inf\n  while (abs(delta) > tol) & (ctr < 50)\n    delta = f(x0)/fp(x0)\n    x0 = x0 - delta\n    ctr = ctr + 1\n  end\n\n  ctr < 50 ? x0 : NaN\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Examples\n\n\n- Find a zero of $\\sin(x)$ starting at $x_0=3$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "nm(sin, cos, 3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an approximation for $\\pi$, that historically found use, as the convergence is fast.\n\n- Find a solution to $x^5 =  5^x$ near $2$:\n\nWriting a function to handle this, we have:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - 5^x"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the derivative, but in this example will let the `D` function from the `Roots` package do so for us:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "alpha = nm(f, f', 2)\nalpha, f(alpha)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions in the Roots package\n\nTyping in the `nm` function might be okay once, but would be tedious\nif it was needed each time. The `Roots` package provides a `Newton`\nmethod. `Roots` is loaded with"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Roots\nfind_zero((sin, cos), 3, Roots.Newton())  # alternatively Roots.newton(sin,cos, 3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, if a derivative is not specified, one can be computed using automatic differentiation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "find_zero((f, f'), 2, Roots.Newton())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The argument `verbose=true` will force a print out of a message summarizing each step.\n\n\nMore generally, the function `find_zero` provides a derivative-free\nalgorithm for finding roots of functions, when started with an initial\nguess. It is similar to Newton's method in that only a good initial\nguess is needed. However, the algorithm, while slower in terms of\nfunction evaluations and steps,  is engineered to be a bit more\nrobust to the choice of initial estimate than Newton's method. (If it\nfinds a bracket, it will use a bisection algorithm which is guaranteed to\nconverge, but can be slower to do so.) Here we see how to call the\nfunction:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = cos(x) - x\nx0 = 1\nfind_zero(f, x0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to this related call which uses the bisection method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "find_zero(f, (0, 1))           ## [0,1] must be a bracketing interval"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example both give the same answer, but the bisection method\nis a bit more inconvenient as a bracketing interval must be pre-specified.\n\n\n##### Example: intersection of two graphs\n\nFind the intersection point between $f(x) = \\cos(x)$ and $g(x) = 5x$ near $0$.\n\nWe have Newton's method to solve for zeros of $f(x)$, i.e. when $f(x) =\n0$. Here we want to solve for $x$ with $f(x) = g(x)$. To do so, we\nmake a new function $h(x) = f(x) - g(x)$, for that is $0$ when $f(x)$\nequals $g(x)$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = cos(x)\ng(x) = 5x\nh(x) = f(x) - g(x)\nx0 = find_zero((h,h'), 0, Roots.Newton())\nx0, h(x0), f(x0), g(x0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example: Finding  $c$ in Rolle's Theorem\n\nThe function $f(x) = \\sqrt{1 - \\cos(x^2)^2}$ has a zero at $0$ and one near 1.77."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = sqrt(1 - cos(x^2)^2)\nplot(f, 0, 1.77)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As $f(x)$ is differentiable between $0$ and $a$, Rolle's theorem says\nthere will be value where the derivative is $0$. Find that value.\n\nThis value will be a zero of the derivative. A graph shows it should be near $1.2$, so we use that as a starting value to get the answer:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "find_zero(f', 1.2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convergence\n\nNewton's method is famously known to have \"quadratic convergence.\" What\ndoes this mean? Let the error in the $i$th step be called $e_i = x_i -\n\\alpha$. Then Newton's method satisfies a bound of the type:\n\n$$~\n\\lvert e_{i+1} \\rvert \\leq M_i \\cdot e_i^2.\n~$$\n\nIf $M$ were just a constant and we suppose $e_0 = 10^{-1}$ then $e_1$\nwould be less than $M 10^{-2}$ and $e_2$ less than $M^2 10^{-4}$,\n$e_3$ less than $M^3 10^{-8}$ and $e_4$ less than $M^4 10^{-16}$ which\nfor $M=1$ is basically the machine precision. That is for some\nproblems, with a good initial guess it will take around 4 or so steps\nto converge.\n\nThe actual value of $M$ depends on $i$ and $f$, so the answer isn't always so easy. To see what $M$ is,\nthe basic assumption of $f$ is such that this fact of linearization holds at each $x_i$ with $f(x_i) \\neq 0$:\n\n$$~\nf(x) = f(x_i) + f'(x_i) \\cdot (x - x_i) + \\frac{1}{2} f''(\\xi) \\cdot (x-x_i)^2.\n~$$\n\nThe value $\\xi$ is from the mean value theorem and is between $x$ and $x_i$.\n\nLet $x=\\alpha$, the zero of $f(x)$ that is being sought. Then\n$f(\\alpha)=0$ and $0=f(x_i)/f'(x_i) + (\\alpha-x_i) + 1/2\\cdot\nf''(\\xi)/f'(x_i) \\cdot (\\alpha-x_i)^2$. For this value, we have\n\n$$~\nx_{i+1} - \\alpha = x_i  - \\frac{f(x_i)}{f'(x_i)} - \\alpha\n= (x_i - \\alpha) + (\\alpha - x_i) + \\frac{1}{2}\\frac{f''(\\xi) \\cdot(\\alpha - x_i)^2}{f'(x_i)}\n=  \\frac{1}{2}\\frac{f''(\\xi)}{f'(x_i)} \\cdot(x_i - \\alpha)^2.\n~$$\n\nThat is\n\n$$~\n\\lvert e_{i+1}\\rvert \\leq \\frac{1}{2}\\frac{\\lvert f''(\\xi)\\rvert}{\\lvert f'(x_i)\\rvert} e_i^2.\n~$$\n\n\nThis convergence will be quadratic *if*:\n\n- The initial guess $x_0$ is not too far from $\\alpha$, so $e_0$ is\n  managed.\n\n- The derivative at $x_i$ is not too close to $0$. (As it appears in\n  the denominator). That is, the function can't be too flat, which\n  should make sense, as then the tangent line is nearly parallel to\n  the $x$ axis and would intersect far away.\n\n- The second derivative is not too big (in absolute value) near the\n  zero. A large second derivative means the function is very concave,\n  which means it is \"turning\" a lot. In this case, the function turns\n  away from the tangent line quickly, so the tangent line's zero is\n  not necessarily a better approximation to the actual zero, $\\alpha$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "note(\"\"\"\nThe basic tradeoff: methods like Newton's are faster than the\nbisection method in terms of function calls, but are not guaranteed to\nconverge, as the bisection method is.\n\"\"\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can go wrong when one of these isn't the case is illustrated next:\n\n### Poor initial step"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### {{{newtons_method_poor_x0}}}\npyplot()\ncaption = \"\"\"\n\nIllustration of Newton's Method converging to a zero of a function,\nbut slowly as the initial guess, is very poor, and not close to the\nzero. The algorithm does converge in this illustration, but not quickly and not to the nearest root from\nthe initial guess.\n\n\"\"\"\n\nfn, a, b, c = x ->  sin(x) - x/4, -15, 20, 2pi\n\nn = 20\nanim = @animate for i=1:n\n    newtons_method_graph(i-1, fn, a, b, c)\nend\n\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 2)\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### {{{newtons_method_flat}}}\npyplot()\ncaption = L\"\"\"\n\nIllustration of Newton's method failing to coverge as for some $x_i$,\n$f'(x_i)$ is too close to 0. In this instance after a few steps, the\nalgorithm just cycles around the local minimum near $0.66$. The values\nof $x_i$ repeat in the pattern: $1.0002, 0.7503, -0.0833, 1.0002,\n\\dots$. This is also an illustration of a poor initial guess. If there\nis a local minimum or maximum between the guess and the zero, such\ncycles can occur.\n\n\"\"\"\n\nfn, a, b, c = x -> x^5 - x + 1, -1.5, 1.4, 0.0\n\nn=7\nanim = @animate for i=1:n\n    newtons_method_graph(i-1, fn, a, b, c)\nend\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 1)\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The second derivative is too big"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### {{{newtons_method_cycle}}}\n\npyplot()\nfn, a, b, c, = x -> abs(x)^(0.49),  -2, 2, 1.0\ncaption = L\"\"\"\n\nIllustration of Newton's Method not converging. Here the second\nderivative is too big near the zero - it blows up near $0$ - and the\nconvergence does not occur. Rather the iterates increase in their\ndistance from the zero.\n\n\"\"\"\n\nn=10\nanim = @animate for i=1:n\n    newtons_method_graph(i-1, fn, a, b, c)\nend\n\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 2)\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The tangent line at some $x_i$ is flat"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "### {{{newtons_method_wilkinson}}}\n\npyplot()\ncaption = L\"\"\"\n\nThe function $f(x) = x^{20} - 1$ has two bad behaviours for Newton's\nmethod: for $x < 1$ the derivative is nearly $0$ and for $x>1$ the\nsecond derivative is very big. In this illustration, we have an\ninitial guess of $x_0=8/9$. As the tangent line is fairly flat, the\nnext approximation is far away, $x_1 = 1.313\\dots$. As this guess is\nis much bigger than $1$, the ratio $f(x)/f'(x) \\approx\nx^{20}/(20x^{19}) = x/20$, so $x_i - x_{i-1} \\approx (19/20)x_i$\nyielding slow, linear convergence until $f''(x_i)$ is moderate. For\nthis function, starting at $x_0=8/9$ takes 11 steps, at $x_0=7/8$\ntakes 13 steps, at $x_0=3/4$ takes 55 steps, and at $x_0=1/2$ it takes\n$204$ steps.\n\n\"\"\"\n\n\nfn,a,b,c = x -> x^20 - 1,  .7, 1.4, 8/9\nn = 10\n\nanim = @animate for i=1:n\n    newtons_method_graph(i-1, fn, a, b, c)\nend\nimgfile = tempname() * \".gif\"\ngif(anim, imgfile, fps = 1)\n\nplotly()\nImageFile(imgfile, caption)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Example\n\nSuppose $\\alpha$ is a simple zero for $f(x)$.  (The value $\\alpha$ is\na zero of multiplicity $k$ if $f(x) = (x-\\alpha)^kg(x)$ where\n$g(\\alpha)$ is not zero.) A simple zero has multiplicity $1$. If\n$f'(\\alpha) \\neq 0$ and the second derivative exists, then a zero\n$\\alpha$ will be simple.)  Around $\\alpha$, quadratic convergence should\napply. However, consider the function $g(x) = f(x)^k$ for some integer\n$k \\geq 2$. Then $\\alpha$ is still a zero, but the derivative of $g$\nat $\\alpha$ is zero, so the tangent line is basically flat. This will\nslow the convergence up. We can see that the update step $g'(x)/g(x)$\nbecomes $(1/k) f'(x)/f(x)$, so an extra factor is introduced.\n\nThe calculation that produces the quadratic convergence now becomes:\n\n$$~\nx_{i+1} - \\alpha = (x_i - \\alpha) - \\frac{1}{k}(x_i-\\alpha + \\frac{f''(\\xi)}{2f'(x_i)}(x_i-\\alpha)^2) =\n\\frac{k-1}{k} (x_i-\\alpha) + \\frac{f''(\\xi)}{2kf'(x_i)}(x_i-\\alpha)^2.\n~$$\n\nAs $k > 1$, the $(x_i - \\alpha)$ term dominates, and we see the\nconvergence is linear with $\\lvert e_{i+1}\\rvert \\approx (k-1)/k\n\\lvert e_i\\rvert$.\n\n\n\n\n\n\n## Questions\n\n###### Question\n\nLook at this graph with $x_0$ marked with a point:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import SpecialFunctions: airyai\np = plot(airyai, -3.3, 0, legend=false);\nplot!(p, zero, -3.3, 0);\nscatter!(p, [-2.8], [0], color=:orange, markersize=5);\nannotate!(p, [(-2.8, 0.2, L\"x_0\")])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If one step of Newton's method was used, what would be the value of $x_1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [L\"-2.224\", L\"-2.80\",  L\"-0.020\", L\"0.355\"]\nans = 1\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLook at this graph of some concave up $f(x)$ with initial point $x_0$ marked. Let $c$ be the zero."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = plot(x -> x^2 - 2, .75, 2.2, legend=false);\nplot!(p, zero,                   color=:green);\nscatter!(p, [1],[0],             color=:orange, markersize=5);\nannotate!(p, [(1,.05, L\"x_0\"), (sqrt(2), .2, L\"c\")]);\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can be said about $x_1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"It must be $x_1 > c$\",\nL\"It must be $x_1 < x_0$\",\nL\"It must be $x_0 < x_1 < c$\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLook at this graph of some concave up $f(x)$ with initial point $x_0$ marked. Let $c$ be the zero."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = plot(x -> x^2 - 2, .75, 2.2, legend=false);\nplot!(p, zero, .75, 2.2, color=:green);\nscatter!(p, [2],[0], color=:orange, markersize=5);\nannotate!(p, [(2,.2, L\"x_0\"), (sqrt(2), .2, L\"c\")]);\np"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can be said about $x_1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"It must be $x_1 < c$\",\nL\"It must be $x_1 > x_0$\",\nL\"It must be $c < x_1 < x_0$\"\n]\nans = 3\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nSuppose $f(x)$ is concave up and we have the tangent line representation: $f(x) = f(c) + f'(c)\\cdot(x-c) + f''(\\xi)/2 \\cdot(x-c)^2$. Explain why it must be that the graph of $f(x)$ lies on or *above* the tangent line."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"As $f''(\\xi)/2 \\cdot(x-c)^2$ is non-negative, we must have $f(x) - (f(c) + f'(c)\\cdot(x-c)) \\geq 0$.\",\nL\"As $f''(\\xi) < 0$ it must be that $f(x) - (f(c) + f'(c)\\cdot(x-c)) \\geq 0$.\",\nL\"This isn't true. The function $f(x) = x^3$ at $x=0$ provides a counterexample\"\n]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n\nLet $f(x) = x^2 - 3^x$. This has derivative $2x - 3^x \\cdot\n\\log(3)$. Starting with $x_0=0$, what does Newton's method converge on?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Roots\nf(x) = x^2 - 3^x;\nfp(x) = 2x - 3^x*log(3);\nval = Roots.newton(f, fp, 0);\nnumericq(val, 1e-14)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n\nLet $f(x) = \\exp(x) - x^4$. There are 3 zeros for this function. Which one does Newton's method converge to when $x_0=2$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = exp(x) - x^4;\nfp(x) = exp(x) - 4x^3;\nxstar= Roots.newton(f, fp, 2);\nnumericq(xstar, 1e-1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n\n\nLet $f(x) = \\exp(x) - x^4$. As mentioned, there are 3 zeros for this function. Which one does Newton's method converge to when $x_0=8$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = exp(x) - x^4;\nfp(x) = exp(x) - 4x^3;\nxstar = Roots.newton(f, fp, 8);\nnumericq(xstar, 1e-1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\n\nLet $f(x) = \\sin(x) - \\cos(4\\cdot x)$.\n\nStarting at $\\pi/8$, solve for the root returned by Newton's method"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "k1=4\nf(x)  = sin(x) - cos(k1*x);\nfp(x) = cos(x) + k1*sin(k1*x);\nval = Roots.newton(f, fp, pi/(2k1));\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nUsing Newton's method find a root to $f(x) = \\cos(x) - x^3$ starting at $x_0 = 1/2$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = cos(x) - x^3\nval = Roots.newton(f,f', 1/2)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nUse Newton's method to find a root of $f(x) = x^5 + x -1$. Make a quick graph to find a reasonable starting point."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 + x - 1\nval = Roots.newton(f,f', -1)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nWill Newton's method converge for the function $f(x) = x^5 - x + 1$ starting at $x=1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"Yes\",\n\"No. The initial guess is not close enough\",\n\"No. The second derivative is too big\",\nL\"No. The first derivative gets too close to $0$ for one of the $x_i$\"]\nans = 2\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nWill Newton's method converge for the function $f(x) = 4x^5 - x + 1$ starting at $x=1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"Yes\",\n\"No. The initial guess is not close enough\",\n\"No. The second derivative is too big, or does not exist\",\nL\"No. The first derivative gets too close to $0$ for one of the $x_i$\"]\nans = 2\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nWill Newton's method converge for the function $f(x) = x^{10} - 2x^3 - x + 1$ starting from $0.25$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"Yes\",\n\"No. The initial guess is not close enough\",\n\"No. The second derivative is too big, or does not exist\",\nL\"No. The first derivative gets too close to $0$ for one of the $x_i$\"]\nans = 1\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nWill Newton's method converge for $f(x) = 20x/(100 x^2 + 1)$ starting at $0.1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"Yes\",\n\"No. The initial guess is not close enough\",\n\"No. The second derivative is too big, or does not exist\",\nL\"No. The first derivative gets too close to $0$ for one of the $x_i$\"]\nans = 4\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nWill Newton's method converge to a zero for $f(x) = \\sqrt{(1 - x^2)^2}$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"Yes\",\n\"No. The initial guess is not close enough\",\n\"No. The second derivative is too big, or does not exist\",\nL\"No. The first derivative gets too close to $0$ for one of the $x_i$\"]\nans = 3\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nUse `find_zero` to find a root of $f(x) = 4x^4 - 5x^3 + 4x^2 -20x -6$ starting at $x_0 = 0$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "val = find_zero(x -> 4x^4 - 5x^3 + 4x^2 -20x -6, 0)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nUse `find_zero` to find a zero of $f(x) = \\sin(x) - x/2$ that is *bigger* than $0$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = sin(x) - x/2\nval = find_zero(f, 2)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nThe Newton baffler (defined below) is so named, as Newton's method will fail to find the root for most starting points."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function newton_baffler(x)\n    if ( x - 0.0 ) < -0.25\n        0.75 * ( x - 0 ) - 0.3125\n    elseif  ( x - 0 ) < 0.25\n        2.0 * ( x - 0 )\n    else\n        0.75 * ( x - 0 ) + 0.3125\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will `find_zero` find the zero at $0.0$ starting at 1 using the default option for `order`?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(\"yes\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will Newton's method find the zero at $0.0$ starting at $1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(\"no\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering this plot:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(newton_baffler, -1.1, 1.1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with $x_0=1$, you can see why Newton's method will fail. Why?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\nL\"It doesn't fail, it converges to $0$\",\nL\"The tangent lines for $|x| > 0.25$ intersect at $x$ values with $|x| > 0.25$\",\nL\"The first derivative is $0$ at $1$\"\n]\nans = 2\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nConsider this crazy function defined by:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "import SpecialFunctions: erf\nf(x) = cos(100*x)-4*erf(30*x-10)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The `erf` function is the error function.)\n\nMake a plot over the interval $[-3,3]$ to see why it is called \"crazy\".\n\nDoes `find_zero` find a zero to this function starting from $0$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(\"yes\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If so, what is the value?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "val = find_zero(f, 0)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not, what is the reason?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"The zero is a simple zero\",\n\"The zero is not a simple zero\",\n\"The function oscillates too much to rely on the tangent line approximation far from the zero\",\n\"We can find an answer\"\n]\nans = 4\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does `find_zero` find a zero to this function starting from $1$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If so, what is the value?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "numericq(-999.999)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not, what is the reason?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "choices = [\n\"The zero is a simple zero\",\n\"The zero is not a simple zero\",\n\"The function oscillates too much to rely on the tangent line approximations far from the zero\",\n\"We can find an answer\"\n]\nans = 3\nradioq(choices, ans, keep_order=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nLet $f(x) = \\sin(x) - x/4$. Starting at  $x_0 = 2\\pi$ Newton's method will converge to a value, but it will take many steps. Using `verbose=true` when calling the `newton` function in the `Roots` package, how many steps does it take:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = sin(x) - x/4\nval = 22\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the zero that is found?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "val = Roots.newton(f,f', 2pi)\nnumericq(val)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this the closest zero to the starting point, $x_0$?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(\"no\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nQuadratic convergence of Newton's method only applies to *simple*\nroots. For example, we can see (using the `verbose=true` argument to\nthe `Roots` package's `newton` method, that it only takes $4$ steps to\nfind a zero to $f(x) = \\cos(x) - x$ starting at $x_0 = 1$. But it takes\nmany more steps to find the same zero for $f(x) = (\\cos(x) - x)^2$.\n\nHow many?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "val = 24\nnumericq(val, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question: implicit equations\n\nThe equation $x^2 + x\\cdot y + y^2 = 1$ is a rotated ellipse."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x,y) = x^2 + x * y + y^2 - 1\nplot(Eq(f, 0), xlims=(-2,2), ylims=(-2,2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we find which point on its graph has the largest $y$ value?\n\nThis would be straightforward *if* we could write $y(x) = \\dots$, for then we would simply find the critical points and investiate. But we can't so easily solve for $y$ interms of $x$. However, we can use Newton's method to do so:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function findy(x)\n  fn = y -> (x^2 + x*y + y^2) - 1\n  fp = y -> (x + 2y)\n  find_zero((fn, fp), sqrt(1 - x^2), Roots.Newton())\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a *fixed* x, this solves for $y$ in the equation: $F(y) = x^2 + x \\cdot y + y^2 - 1 = 0$. It should be that $(x,y)$ is a solution:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = .75\ny = findy(x)\nx^2 + x*y + y^2  ## is this 1?"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have a means to find $y(x)$, but it is implicit. We can't readily find the derivative to find critical points. Instead we can use the approximate derivative with $h=10^{-6}$:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yp(x) = (findy(x + 1e-6) - findy(x)) / 1e-6"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `find_zero`, find the value $x$ which maximizes `yp`. Use this to find the point $(x,y)$ with largest $y$ value."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstar = find_zero(yp, 0.5)\nystar = findy(xstar)\nchoices = [L\"(-0.577, 1.155)\", L\"(0,0)\", L\"(0, -0.577)\", L\"(0.577, 0.577)\"]\nans = 1\nradioq(choices, ans)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Question\n\nIn the last problem we used an *approximate* derivative in place of the derivative. This can introduce an error due to the approximation. Will this be true if we replace the derivative in Newton's method with an approximation? In general, this can often be done *but* the convergence can be *slower* and the sensitivity to a poor initial guess even greater.\n\nThree common approximations are given by the\ndifference quotient for a fixed $h$: $f'(x_i) \\approx (f(x_i+h)-f(x_i))/h$;\nthe secant line approximation: $f'(x_i) \\approx (f(x_i) - f(x_{i-1})) / (x_i - x_{i-1})$; and the\nSteffensen approximation $f'(x_i) \\approx (f(x_i + f(x_i)) - f(x_i)) / f(x_i)$ (using $h=f(x_i)$).\n\n\nLet's revisit the $4$-step convergence of Newton's method to the root of $f(x) = 1/x - q$ when $q=0.8$. Will these methods be as fast?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "q = 0.8\nxstar = 1.25 # q = 4/5 --> 1/q = 5/4\nf(x) = 1/x - q"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the above approximations for a given `f`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "delta = 1e-6\nsecant_approx(x0,x1) = (f(x1) - f(x0)) / (x1 - x0)\ndiffq_approx(x0, h) = secant_approx(x0, x0+h)\nsteff_approx(x0) = diffq_approx(x0, f(x0))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then using the difference quotient would look like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x1 = 42/17 - 32/17*q\nx1 = x1 - f(x1) / diffq_approx(x1, delta)   # |x1 - xstar| = 0.06511395862036995\nx1 = x1 - f(x1) / diffq_approx(x1, delta)   # |x1 - xstar| = 0.003391809999860218; etc"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Steffensen method would look like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x1 = 42/17 - 32/17*q\nx1 = x1 - f(x1) / steff_approx(x1)   # |x1 - xstar| = 0.011117056291670258\nx1 = x1 - f(x1) / steff_approx(x1)   # |x1 - xstar| = 3.502579696146313e-5; etc."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the secant method like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x1 = 42/17 - 32/17*q\nx0 = x1 - delta # we need two initial values\nx0, x1 = x1, x1 - f(x1) / secant_approx(x0, x1)   # |x1 - xstar| = 8.222358365284066e-6\nx0, x1 = x1, x1 - f(x1) / secant_approx(x0, x1)   # |x1 - xstar| = 1.8766323799379592e-6; etc."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat each of the above algorithms until `abs(x1 - 1.25)` is `0` (which will happen for this problem, though not in general). Record the steps.\n\n* Does the difference quotient need *more* than $4$ steps?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Does the secant method need *more* than $4$ steps?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Does the Steffensen method need *more* than 4 steps?"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "yesnoq(false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "All methods work quickly with this well-behaved problem. In general\nthe convergence rates are slightly different for each, with the\nSteffensen method matching Newton's method and the difference quotient\nmethod being slower in general. All can be more sensitive to the initial guess."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.0"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
